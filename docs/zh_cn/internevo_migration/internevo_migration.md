# æ€»è§ˆ

XTuner å¯ä»¥å¤ç° InternEvo (train_internlm) ä»“åº“è®­ç»ƒå¾—åˆ°çš„å¼€æºæ¨¡å‹ internlm/internlm2-chat-7b çš„è®­ç»ƒç²¾åº¦ã€‚

ä¸‹é¢æ˜¯ XTuner å’Œ InternEvo (train_internlm) åœ¨ç›¸åŒæ•°æ®é›†ä¸Šè®­ç»ƒç›¸åŒåŸºåº§æ¨¡å‹çš„è®­ç»ƒç»“æœå¯¹æ¯”ï¼š

|        èƒ½åŠ›ç±»åˆ«        | xtuner | internevo |
| :--------------------: | :----: | :-------: |
| å…¨æ•°æ®é›†å¹³å‡(æ— æ™ºèƒ½ä½“) | 56.44  |   55.26   |
|  å…¨ç»´åº¦å¹³å‡(æ— æ™ºèƒ½ä½“)  | 49.58  |   48.96   |
|     è¯­è¨€ Language      | 64.77  |   62.41   |
|     çŸ¥è¯† Knowledge     | 52.24  |   52.52   |
|     æ¨ç† Reasoning     |  65.5  |   63.91   |
|    æ•°å­¦ Mathematics    | 30.95  |   30.26   |
|      ä»£ç  Coding       | 38.91  |   41.06   |
|    é•¿æ–‡æœ¬ LongEval     | 45.09  |   43.62   |
|      æ™ºèƒ½ä½“ Agent      | 44.85  |   43.97   |
|      æ•°å­¦é¢˜æ™ºèƒ½ä½“      |   37   |   37.19   |
|        CIBench         | 79.07  |   69.78   |
|       PluginEval       | 65.57  |   65.62   |

64 * A100 çš„è®­ç»ƒæ—¶é—´å¯¹æ¯”å¦‚ä¸‹ï¼š

|   xtuner    | internevo  |
| :---------: | :--------: |
| 15 h 55 min | 16h 09 min |

æ³¨ï¼šä½¿ç”¨ XTuner æä¾›çš„åºåˆ—å¹¶è¡Œç®—æ³•å¯ä»¥è¿›ä¸€æ­¥æå‡è®­ç»ƒé€Ÿåº¦ï¼Œä½¿ç”¨æ–¹å¼è¯·å‚è€ƒ [åºåˆ—å¹¶è¡Œæ–‡æ¡£](../training/training_extreme_long_sequence.md) ã€‚

åœ¨ä» InternEvo (train_internlm) å‘ XTuner è¿ç§»çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬éœ€è¦å…³æ³¨æ¨¡å‹ã€æ•°æ®ä»¥åŠè®­ç»ƒç­–ç•¥è¿™ä¸‰ä¸ªæ–¹é¢çš„é€‚é…é—®é¢˜ã€‚åç»­å†…å®¹å°†è¯¦ç»†é˜è¿°å¦‚ä½•è¿›è¡Œé€‚é…ã€‚

# é€‚é…

## æ¨¡å‹

InternEvo åœ¨è®­ç»ƒæ—¶è¯»å–å’Œä¿å­˜çš„æ¨¡å‹æƒé‡æ»¡è¶³ä»¥ä¸‹ç›®å½•ç»“æ„ï¼ˆä»¥ tp2pp2 ä¸ºä¾‹ï¼‰ï¼š

```
|-- root
    |-- model_config.pt
    |-- model_tp0_pp0.pt
    |-- model_tp0_pp1.pt
    |-- model_tp1_pp0.pt
    |-- model_tp1_pp1.pt
```

å…¶ä¸­ï¼Œ`model_config.pt` ä¿å­˜æ¨¡å‹æƒé‡çš„ä¸€äº› meta ä¿¡æ¯ï¼Œå…¶ä½™ 4 ä¸ª checkpoint åˆ™åˆ†åˆ«ä¿å­˜ 4 ç»„ GPUs ä¸Šçš„æ¨¡å‹æƒé‡ã€‚å› æ­¤ï¼ŒInternEvo è®­ç»ƒè¿‡ç¨‹ä¸­è¦æ±‚è¯»å–é¢„è®­ç»ƒæƒé‡çš„ tpã€pp ç­–ç•¥ä¸è®­ç»ƒä½¿ç”¨çš„ tpã€pp ç­–ç•¥ä¸€è‡´æ‰èƒ½æ­£å¸¸è¯»å–é¢„è®­ç»ƒæƒé‡è¿›è¡Œè®­ç»ƒã€‚

XTuner æ”¯æŒåŸºäº Huggingface Hub ä¸Šçš„æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå¦‚ä¸‹ä¿®æ”¹ config å†…å®¹å³å¯å°†åŸºåº§æ¨¡å‹ä» internlm2-7b åˆ‡æ¢ä¸º internlm2-20bï¼š

```diff
#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Model
- pretrained_model_name_or_path = 'internlm/internlm2-7b'
+ pretrained_model_name_or_path = 'internlm/internlm2-20b'

```

## æ•°æ®

InternEvo åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šå¸¸ä¼šæŠŠå¤šæ¡æ•°æ®æ‹¼æ¥ä¸ºä¸€ä¸ªç‰¹å®šçš„æœ€å¤§é•¿åº¦ï¼Œéšåè¾“å…¥æ¨¡å‹è®­ç»ƒã€‚å…¶é…ç½®å¾€å¾€æ»¡è¶³ä»¥ä¸‹å½¢å¼ï¼š

```python
data = dict(
    seq_len=SEQ_LEN,
    pack_sample_into_one=False,
    min_length=MIN_LENGTH,
    train_folder=TRAIN_FOLDER,
    dataset_weights=DATASET_WEIGHTS,
    ...)
```

å…¶ä¸­ï¼Œæ•°æ®é…æ¯” (`dataset_weights=DATASET_WEIGHTS`) åŠŸèƒ½ XTuner å°šæœªæ”¯æŒã€‚`TRAIN_FOLDER` ä¸­çš„è®­ç»ƒæ•°æ®éœ€è¦æ»¡è¶³ ftdp tokenized æ•°æ®é›†æ ¼å¼ï¼š

```
|-- TRAIN_FOLDER
    |-- cn
    |   |-- dataset1
    |   |   |-- data1.bin
    |   |   |-- data1.bin.meta
    |   |-- dataset2
    |   |   |-- data2.bin
    |   |   |-- data2.bin.meta
```

åœ¨ XTuner ä¸­å®ç°åœ¨çº¿æ•°æ®é›†æ‹¼æ¥ç­–ç•¥éœ€è¦å‚è€ƒ `xtuner/configs/internlm/internlm2_7b/internlm2_7b_w_internevo_dataset.py` æ–‡ä»¶ä¸­çš„é…ç½®ï¼š

```diff
#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Data
- dataset_folder = '/path/to/sft/data/folder'
+ dataset_folder = TRAIN_FOLDER
- max_length = 32768
+ max_length = SEQ_LEN

#######################################################################
#                      PART 3  Dataset & Dataloader                   #
#######################################################################
train_dataset = dict(
    type=build_packed_dataset,
    dataset_cfg=dict(
        type=load_intern_repo_tokenized_dataset,
        data_order_path=None,
        folder=dataset_folder,
-       min_length=0,
+       min_length=MIN_LENGTH,
        file_type='.bin'),
    packed_length=max_length,
    seed=1024)
```

> \[!IMPORTANT\]
> éœ€è¦æ³¨æ„ï¼Œç”±äºè®­ç»ƒæ•°æ®å–‚ç»™æ¨¡å‹çš„å…ˆåé¡ºåºå¯èƒ½å¯¹è®­ç»ƒç»“æœé€ æˆå½±å“ï¼Œå› æ­¤å»ºè®®ä¸è¦è½»æ˜“ä¿®æ”¹ä¸Šè¿°é…ç½®ä¸­çš„ `seed` é€‰é¡¹ã€‚åŒæ—¶ï¼Œå¯å‚è€ƒ[æ–‡æ¡£todo](./ftdp_dataset/Case4.md#step-3-è·å–æ•°æ®é¡ºåº-å¯é€‰)è¿›ä¸€æ­¥å›ºå®šæ•°æ®é¡ºåºã€‚

## è®­ç»ƒç­–ç•¥

### å˜é•¿æ³¨æ„åŠ› (Variable Length Flash Attention)

InternEvo é€šè¿‡è®¾ç½® [æ•°æ®é…ç½®](https://github.com/InternLM/InternEvo/blob/77c3b46bfe51f6bc245c4aba98639221b8618372/doc/usage.md#%E6%95%B0%E6%8D%AE%E9%85%8D%E7%BD%AE) ä¸­çš„ `pack_sample_into_one` å‚æ•°ä¸º False æ¥ä½¿ç”¨â€œå˜é•¿æ³¨æ„åŠ›æœºåˆ¶â€ï¼ˆè§ä¸‹å›¾å³ä¾§ï¼‰ã€‚

```python
data = dict(
    pack_sample_into_one=False,
    ...)
```

<div align="center">
  <img src="https://github.com/InternLM/InternEvo/blob/develop/doc/imgs/pack_into_one.png?raw=true" width="800"/>
  <br /><br />
</div>

åœ¨ XTuner ä¸­ä½¿ç”¨è¿™ä¸€åŠŸèƒ½éœ€è¦è®¾ç½® config ä¸­çš„ `use_varlen_attn` é…ç½®ä¸º Trueï¼Œå³å¯ä¿è¯è®­ç»ƒè¡Œä¸ºä¸ InternEvo ä¸€è‡´ï¼š

```diff
...
#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Model
pretrained_model_name_or_path = 'internlm/internlm2-7b'
- use_varlen_attn = False
+ use_varlen_attn = True
...
```

> \[!IMPORTANT\]
> éœ€è¦æ³¨æ„ï¼Œå½“è®¾ç½® `use_varlen_attn = True` åï¼Œè¯·ç¡®ä¿ `batch_size` è¢«è®¾ç½®ä¸º 1ï¼Œä¸” `pack_to_max_length` è¢«è®¾ç½®ä¸º Trueã€‚

### batch_size ä¸ accumulative_counts

åœ¨ InternEvo çš„é…ç½®ä¸­ï¼Œä¸ batch_size å’Œ accumulative_counts ç›¸å…³çš„é…ç½®æœ‰å¦‚ä¸‹å‡ ä¸ªï¼š

```python
data = dict(
    # micro_num means the number of micro_batch contained in one gradient update
    micro_num=MICRO_NUM,
    # MICRO_BATCH_SIZE * SEQ_LEN = PACKED_LENGTH
    micro_bsz=MICRO_BATCH_SIZE,
    total_steps=TOTAL_STEP,
    # æ¢¯åº¦ç´¯è®¡ï¼Œé»˜è®¤ç­‰äºMICRO_NUMï¼ˆBSï¼‰
    gradient_accumulation=GRADIENT_ACCUMULATION,
    ...)
```

å…¶ä¸­ï¼š

1. `micro_num` ä¸ `gradient_accumulation` é€šå¸¸å…·æœ‰ç›¸åŒå«ä¹‰ï¼Œå…¶æ•°å€¼é»˜è®¤ç›¸ç­‰ã€‚
2. `total_steps` åœ¨ XTuner ä¸­å¯ä»¥ä¸æ‰‹åŠ¨æŒ‡å®šï¼Œå¯é€šè¿‡ `max_epochs` æŒ‡å®šã€‚
3. XTuner ç›®å‰åªæ”¯æŒ `micro_bsz = 1` ã€‚

ä¸ºå¯¹é½ä»¥ä¸Šé…ç½®ï¼Œå¯å‚è€ƒ XTuner ä¸­ `xtuner/configs/internlm/internlm2_7b/internlm2_7b_w_internevo_dataset.py` æ–‡ä»¶ä¸­çš„é…ç½®ï¼Œå¹¶è¿›è¡Œå¦‚ä¸‹ä¿®æ”¹ï¼š

```diff
#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Scheduler & Optimizer
- accumulative_counts = 1
+ accumulative_counts = MICRO_NUM # or GRADIENT_ACCUMULATION
- max_epochs = 1
+ max_epochs = MAX_EPOCHS
```

### å¹¶è¡Œè®­ç»ƒ

#### ZeRO ç³»åˆ—æ˜¾å­˜ä¼˜åŒ–

XTuner æ”¯æŒä½¿ç”¨ ZeRO ç³»åˆ—æ˜¾å­˜ä¼˜åŒ–é™ä½è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ˜¾å­˜æ¶ˆè€—ï¼š

```shell
  # å•å¡
  xtuner train ${CONFIG_NAME_OR_PATH} --deepspeed deepspeed_zero2
  # å¤šå¡
  (DIST) NPROC_PER_NODE=${GPU_NUM} xtuner train ${CONFIG_NAME_OR_PATH} --deepspeed deepspeed_zero2
  (SLURM) srun ${SRUN_ARGS} xtuner train ${CONFIG_NAME_OR_PATH} --launcher slurm --deepspeed deepspeed_zero2
```

- `--deepspeed` è¡¨ç¤ºä½¿ç”¨ [DeepSpeed](https://github.com/microsoft/DeepSpeed) ğŸš€ æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚XTuner å†…ç½®äº†å¤šç§ç­–ç•¥ï¼ŒåŒ…æ‹¬ ZeRO-1ã€ZeRO-2ã€ZeRO-3 ã€‚

#### åºåˆ—å¹¶è¡Œ

InternEvo ä¸­æ”¯æŒäº† Data Parallelã€Tensor Parallelã€Pipeline Parallel å’Œ Sequence Parallel å››ç§å¹¶è¡Œç­–ç•¥ã€‚XTuner ç›®å‰æ”¯æŒäº† Data Parallel å’Œ Sequence Parallel ä¸¤ç§å¹¶è¡Œç­–ç•¥ï¼Œå¯æ»¡è¶³åŸºæœ¬å…¨éƒ¨çš„è®­ç»ƒéœ€æ±‚ï¼ˆæ­é… zero3 æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥å¯æ”¯æŒ 70B æ¨¡å‹ 256K ä¸Šä¸‹æ–‡è®­ç»ƒï¼‰ã€‚

å‡å®š InternEvo è®­ç»ƒè¿‡ç¨‹ä¸­ï¼štp_world_size = TP, pp_world_size = PP, sequence_parallel = Trueã€‚åˆ™è®­ç»ƒçš„ global_batch_size æ»¡è¶³ä»¥ä¸‹è®¡ç®—å…¬å¼:

```
# å¤šé™¤çš„ä¸€ä¸ª TP æ˜¯å› ä¸ºå¯ç”¨äº† sequence parallel
global_batch_size = num_gpus * batch_size_per_device * gradient_accumulate / TP / PP / TP
```

éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œinternlm2-chat çš„è®­ç»ƒè¿‡ç¨‹ä¸­é€šå¸¸å¯ç”¨äº† [â€œå˜é•¿æ³¨æ„åŠ›â€](#å˜é•¿æ³¨æ„åŠ›-variable-length-flash-attention) ç­–ç•¥ï¼Œæ­¤æ—¶ `å•å¡ batch size ç­‰äº 2ï¼Œæ‹¼æ¥æ•°æ®é›†è‡³æœ€å¤§é•¿åº¦ 2k` çš„é…ç½®ä¸ `å•å¡ batch size ç­‰äº 1ï¼Œæ‹¼æ¥æ•°æ®é›†è‡³æœ€å¤§é•¿åº¦ 4k` çš„é…ç½®è®­ç»ƒè¡Œä¸ºæ˜¯è¿‘ä¼¼çš„ï¼Œå› æ­¤ XTuner ç›®å‰åªæ”¯æŒäº† `batch_size_per_device = 1` çš„æƒ…å†µã€‚å› æ­¤ï¼Œè‹¥æƒ³ä½¿ç”¨ XTuner è®­ç»ƒæ—¶ä¿è¯ global_batch_size ä¸ InternEvo ä¸€è‡´ï¼Œéœ€è¦åœ¨é…ç½®æ–‡ä»¶ä¸­ç»¼åˆè°ƒæ•´ `gradient_accumulate` å’Œ `sequence_parallel_size` ä¸¤é¡¹çš„æ•°å€¼ï¼š

```diff
+ from xtuner.parallel.sequence import SequenceParallelSampler

+ sequence_parallel_size = SP
- accumulative_counts = 1  # 1bs * 1acc * 64gpu = 64 batchsize
+ accumulative_counts = TP * PP * TP / SP

#######################################################################
#                      PART 3  Dataset & Dataloader                   #
#######################################################################
train_dataloader = dict(
-   sampler=dict(type=DefaultSampler, shuffle=True),
+   sampler=dict(type=SequenceParallelSampler, shuffle=True),
    ...)
```

XTuner åºåˆ—å¹¶è¡Œçš„è¯¦ç»†ç”¨æ³•è¯·å‚è€ƒ [åºåˆ—å¹¶è¡Œæ–‡æ¡£](../training/training_extreme_long_sequence.md)ã€‚
