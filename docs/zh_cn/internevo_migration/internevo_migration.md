# æ¨¡å‹

XTuner æ”¯æŒåŸºäº Huggingface Hub ä¸Šçš„æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œå¦‚ä¸‹ä¿®æ”¹ config å†…å®¹å³å¯åˆ‡æ¢æ¨¡å‹ï¼š

```diff
#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Model
- pretrained_model_name_or_path = 'internlm/internlm2-7b'
+ pretrained_model_name_or_path = 'internlm/internlm2-20b'

```

# æ•°æ®

InternEvo åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šå¸¸ä¼šæŠŠå¤šæ¡æ•°æ®æ‹¼æ¥ä¸ºä¸€ä¸ªç‰¹å®šçš„æœ€å¤§é•¿åº¦ï¼Œéšåè¾“å…¥æ¨¡å‹è®­ç»ƒã€‚å…¶é…ç½®å¾€å¾€æ»¡è¶³ä»¥ä¸‹å½¢å¼ï¼š

```python
data = dict(
    seq_len=SEQ_LEN,
    pack_sample_into_one=False,
    min_length=MIN_LENGTH,
    train_folder=TRAIN_FOLDER,
    dataset_weights=DATASET_WEIGHTS,
    ...)
```

å…¶ä¸­ï¼Œæ•°æ®é…æ¯” (`dataset_weights=DATASET_WEIGHTS`) åŠŸèƒ½ XTuner å°šæœªæ”¯æŒã€‚`TRAIN_FOLDER` ä¸­çš„è®­ç»ƒæ•°æ®éœ€è¦æ»¡è¶³ ftdp tokenized æ•°æ®é›†æ ¼å¼ï¼š

```
|-- TRAIN_FOLDER
    |-- cn
    |   |-- dataset1
    |   |   |-- data1.bin
    |   |   |-- data1.bin.meta
    |   |-- dataset2
    |   |   |-- data2.bin
    |   |   |-- data2.bin.meta
```

åœ¨ XTuner ä¸­å®ç°åœ¨çº¿æ•°æ®é›†æ‹¼æ¥ç­–ç•¥éœ€è¦å‚è€ƒ `xtuner/configs/internlm/internlm2_7b/internlm2_7b_w_tokenized_dataset.py` æ–‡ä»¶ä¸­çš„é…ç½®ï¼š

```diff
#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Data
- dataset_folder = '/path/to/sft/data/folder'
+ dataset_folder = TRAIN_FOLDER
- max_length = 32768
+ max_length = SEQ_LEN

#######################################################################
#                      PART 3  Dataset & Dataloader                   #
#######################################################################
train_dataset = dict(
    type=build_packed_dataset,
    dataset_cfg=dict(
        type=load_intern_repo_tokenized_dataset,
        data_order_path=None,
        folder=dataset_folder,
-       min_length=0,
+       min_length=MIN_LENGTH,
        file_type='.bin'),
    packed_length=max_length,
    seed=1024)
```

> \[!IMPORTANT\]
> éœ€è¦æ³¨æ„ï¼Œç”±äºè®­ç»ƒæ•°æ®å–‚ç»™æ¨¡å‹çš„å…ˆåé¡ºåºå¯èƒ½å¯¹è®­ç»ƒç»“æœé€ æˆå½±å“ï¼Œå› æ­¤å»ºè®®ä¸è¦è½»æ˜“ä¿®æ”¹ä¸Šè¿°é…ç½®ä¸­çš„ `seed` é€‰é¡¹ã€‚åŒæ—¶ï¼Œå¯å‚è€ƒ[æ–‡æ¡£todo](./ftdp_dataset/Case4.md#step-3-è·å–æ•°æ®é¡ºåº-å¯é€‰)è¿›ä¸€æ­¥å›ºå®šæ•°æ®é¡ºåºã€‚

# è®­ç»ƒç­–ç•¥

## å˜é•¿æ³¨æ„åŠ› (Variable Length Flash Attention)

InternEvo é€šè¿‡è®¾ç½®[æ•°æ®é…ç½®](https://github.com/InternLM/InternEvo/blob/develop/doc/usage.md#%E6%95%B0%E6%8D%AE%E9%85%8D%E7%BD%AE)ä¸­çš„ `pack_sample_into_one` å‚æ•°ä¸º False æ¥ä½¿ç”¨â€œå˜é•¿æ³¨æ„åŠ›æœºåˆ¶â€ï¼ˆè§ä¸‹å›¾å³ä¾§ï¼‰ã€‚

<div align="center">
  <img src="https://github.com/InternLM/InternEvo/blob/develop/doc/imgs/pack_into_one.png?raw=true" width="800"/>
  <br /><br />
</div>

åœ¨ XTuner ä¸­ä½¿ç”¨è¿™ä¸€åŠŸèƒ½éœ€è¦è®¾ç½® config ä¸­çš„ `use_varlen_attn` é…ç½®ä¸º Trueï¼Œå³å¯ä¿è¯è®­ç»ƒè¡Œä¸ºä¸ InternEvo ä¸€è‡´ï¼š

```diff
...
#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Model
pretrained_model_name_or_path = 'internlm/internlm2-7b'
- use_varlen_attn = False
+ use_varlen_attn = True
...
```

> \[!IMPORTANT\]
> éœ€è¦æ³¨æ„ï¼Œå½“è®¾ç½® `use_varlen_attn = True` åï¼Œè¯·ç¡®ä¿ `batch_size` è¢«è®¾ç½®ä¸º 1ï¼Œä¸” `pack_to_max_length` è¢«è®¾ç½®ä¸º Trueã€‚

## batch_size ä¸ accumulative_counts

åœ¨ InternEvo çš„é…ç½®ä¸­ï¼Œä¸ batch_size å’Œ accumulative_counts ç›¸å…³çš„é…ç½®æœ‰å¦‚ä¸‹å‡ ä¸ªï¼š

```python
data = dict(
    # micro_num means the number of micro_batch contained in one gradient update
    micro_num=MICRO_NUM,
    # MICRO_BATCH_SIZE * SEQ_LEN = PACKED_LENGTH
    micro_bsz=MICRO_BATCH_SIZE,
    total_steps=TOTAL_STEP,
    # æ¢¯åº¦ç´¯è®¡ï¼Œé»˜è®¤ç­‰äºMICRO_NUMï¼ˆBSï¼‰
    gradient_accumulation=GRADIENT_ACCUMULATION,
    ...)
```

å…¶ä¸­ï¼š

1. `micro_num` ä¸ `gradient_accumulation` é€šå¸¸å…·æœ‰ç›¸åŒå«ä¹‰ï¼Œå…¶æ•°å€¼é»˜è®¤ç›¸ç­‰ã€‚
2. `total_steps` åœ¨ XTuner ä¸­å¯ä»¥ä¸æ‰‹åŠ¨æŒ‡å®šï¼Œå¯é€šè¿‡ `max_epochs` æŒ‡å®šã€‚
3. XTuner ç›®å‰åªæ”¯æŒ `micro_bsz = 1` ã€‚

ä¸ºå¯¹é½ä»¥ä¸Šé…ç½®ï¼Œå¯å‚è€ƒ XTuner ä¸­ `xtuner/configs/internlm/internlm2_7b/internlm2_7b_w_tokenized_dataset.py` æ–‡ä»¶ä¸­çš„é…ç½®ï¼Œå¹¶è¿›è¡Œå¦‚ä¸‹ä¿®æ”¹ï¼š

```diff
#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Scheduler & Optimizer
- accumulative_counts = 1
+ accumulative_counts = MICRO_NUM # or GRADIENT_ACCUMULATION
- max_epochs = 1
+ max_epochs = MAX_EPOCHS
```

## å¹¶è¡Œè®­ç»ƒ

### ZeRO ç³»åˆ—æ˜¾å­˜ä¼˜åŒ–

XTuner æ”¯æŒä½¿ç”¨ ZeRO ç³»åˆ—æ˜¾å­˜ä¼˜åŒ–é™ä½è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ˜¾å­˜æ¶ˆè€—ï¼š

```shell
  # å•å¡
  xtuner train ${CONFIG_NAME_OR_PATH} --deepspeed deepspeed_zero2
  # å¤šå¡
  (DIST) NPROC_PER_NODE=${GPU_NUM} xtuner train ${CONFIG_NAME_OR_PATH} --deepspeed deepspeed_zero2
  (SLURM) srun ${SRUN_ARGS} xtuner train ${CONFIG_NAME_OR_PATH} --launcher slurm --deepspeed deepspeed_zero2
```

- `--deepspeed` è¡¨ç¤ºä½¿ç”¨ [DeepSpeed](https://github.com/microsoft/DeepSpeed) ğŸš€ æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚XTuner å†…ç½®äº†å¤šç§ç­–ç•¥ï¼ŒåŒ…æ‹¬ ZeRO-1ã€ZeRO-2ã€ZeRO-3 ã€‚

### åºåˆ—å¹¶è¡Œ

InternEvo ä¸­æ”¯æŒäº† Data Parallelã€Tensor Parallelã€Pipeline Parallel å’Œ Sequence Parallel å››ç§å¹¶è¡Œç­–ç•¥ã€‚XTuner ç›®å‰æ”¯æŒäº† Data Parallel å’Œ Sequence Parallel ä¸¤ç§å¹¶è¡Œç­–ç•¥ï¼Œå¯æ»¡è¶³åŸºæœ¬å…¨éƒ¨çš„è®­ç»ƒéœ€æ±‚ï¼ˆæ­é… zero3 æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥å¯æ”¯æŒ 70B æ¨¡å‹ 256K ä¸Šä¸‹æ–‡è®­ç»ƒï¼‰ã€‚

å‡å®š InternEvo è®­ç»ƒè¿‡ç¨‹ä¸­ï¼štp_world_size = TP, pp_world_size = PP, sequence_parallel = Trueã€‚åˆ™åœ¨ XTuner çš„é…ç½®æ–‡ä»¶ä¸­è¿›è¡Œå¦‚ä¸‹ä¿®æ”¹å¯ä¿è¯è®­ç»ƒè¡Œä¸ºä¸€è‡´ï¼š

```diff
+ from xtuner.parallel.sequence import SequenceParallelSampler

+ sequence_parallel_size = TP * PP

#######################################################################
#                      PART 3  Dataset & Dataloader                   #
#######################################################################
train_dataloader = dict(
-   sampler=dict(type=DefaultSampler, shuffle=True),
+   sampler=dict(type=SequenceParallelSampler, seed=1024, shuffle=True),
    ...)
```

XTuner åºåˆ—å¹¶è¡Œçš„è¯¦ç»†ç”¨æ³•è¯·å‚è€ƒ [åºåˆ—å¹¶è¡Œæ–‡æ¡£](../training/training_extreme_long_sequence.md)ã€‚
