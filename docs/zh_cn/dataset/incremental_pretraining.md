# å¢é‡é¢„è®­ç»ƒdata pipeline

å¢é‡é¢„è®­ç»ƒæ—¨åœ¨æå‡æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸæˆ–ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå…¶æ•°æ®å¤„ç†æµç¨‹å¯ä»¥åˆ†ä¸ºï¼š

1. æ‰©å……tokenizerè¯è¡¨å¤§å°ï¼ˆå¯é€‰ï¼‰
2. æŒ‰ç…§ç›¸åº”æ•°æ®é›†æ ¼å¼æ„å»ºæ•°æ®é›†

## æ‰©å……tokenizerè¯è¡¨ï¼ˆå¯é€‰ï¼Œæ­£åœ¨å¼€å‘ä¸­Â·Â·Â·ï¼‰

ğŸ’¡ ä¸ºäº†é€‚åº”è¯è¡¨æ‰©å±•æ‰€å¸¦æ¥çš„æ¨¡å‹æƒé‡ç»´åº¦å˜åŒ–ï¼ŒxTunerä¼šæ ¹æ®å­—è¡¨å¤§å°è‡ªåŠ¨è°ƒæ•´è¯­è¨€æ¨¡å‹çš„token embedding layerå’Œlm headçš„å‚æ•°ç»´åº¦ã€‚ç”±äºè¿™ä¸ªæ“ä½œä¼šå¼•å…¥ä¸€äº›éšæœºåˆå§‹åŒ–çš„å‚æ•°ï¼Œå› æ­¤é€šå¸¸éœ€è¦åœ¨æ›´å¤§çš„è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒä»¥è·å¾—æ›´å¥½çš„æ•ˆæœã€‚

## æ•°æ®é›†æ„å»º

xTunerå·²ç»æ”¯æŒä½¿ç”¨ä»¥ä¸‹æ•°æ®é›†è¿›è¡Œè®­ç»ƒï¼š

- [oasst1](https://huggingface.co/datasets/OpenAssistant/oasst1)ï¼šå¢é‡é¢„è®­ç»ƒæ•°æ®é›†ï¼Œå¤šè½®å¯¹è¯æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†
- [alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)ï¼šå•è½®å¯¹è¯æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†
- [alpaca_zh](https://huggingface.co/datasets/silk-road/alpaca-data-gpt4-chinese)ï¼šå•è½®å¯¹è¯æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†
- [openorca](https://huggingface.co/datasets/Open-Orca/OpenOrca)ï¼šå•è½®å¯¹è¯æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†
- [arxiv](https://kaggle.com/datasets/Cornell-University/arxiv)ï¼šå•è½®å¯¹è¯æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œæ•°æ®é›†ä¸­åŒ…å«arxivæ–‡ç« æ‘˜è¦ä¸å¯¹åº”æ ‡é¢˜
- [cmd](https://github.com/Toyhom/Chinese-medical-dialogue-data/raw/master/Data_æ•°æ®/)ï¼šå•è½®å¯¹è¯æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†ï¼Œæ•°æ®é›†ä¸­åŒ…å«åŒ»ç–—ç›¸å…³æ•°æ®
- [moss](https://huggingface.co/datasets/fnlp/moss-003-sft-data)ï¼šå·¥å…·ä½¿ç”¨æ•°æ®é›†

è‹¥è¦ä½¿ç”¨å…¶ä»–å·²æœ‰æ•°æ®é›†æˆ–è‡ªå®šä¹‰æ•°æ®é›†è¿›è¡ŒSFTï¼ˆSupervised FineTuneï¼‰ï¼Œå¯ä»¥å‚è€ƒä¸‹é¢çš„æ–‡æ¡£ã€‚

### ä½¿ç”¨å…¶ä»–å·²æœ‰æ•°æ®é›†

#### Step1 åˆ—å‡ºå€™é€‰æ¨¡å‹åå­—

```bash
xtuner list-cfg -p internlm
```

`-p`ä¸ºæ¨¡ç³ŠæŸ¥æ‰¾ï¼Œè‹¥æƒ³è®­ç»ƒå…¶ä»–æ¨¡å‹ï¼Œå¯ä»¥ä¿®æ”¹`internlm`ä¸ºxtuneræ”¯æŒçš„å…¶ä»–æ¨¡å‹åç§°ã€‚

#### Step2 å¤åˆ¶configæ–‡ä»¶

```bash
xtuner copy-cfg internlm_7b_qlora_oasst1_e3 xtuner/configs/internlm/internlm_7b/
```

#### Step 3 ä¿®æ”¹configæ–‡ä»¶

step2å¤åˆ¶å¾—åˆ°çš„configæ–‡ä»¶å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from xtuner.datasets import process_hf_dataset
from datasets import load_dataset
################ Modification 1 ###########
from xtuner.datasets.map_fns import oasst1_map_fn
############################################
...
#######################################################################
#                          STEP 1  Settings                           #
#######################################################################
...
#######################################################################
#                      STEP 2  Model & Tokenizer                      #
#######################################################################
...
#######################################################################
#                      STEP 3  Dataset & Dataloader                   #
#######################################################################
train_dataset = dict(
    type=process_hf_dataset,
    ################ Modification 2 ###########
    dataset=dict(type=load_dataset, path=data_path),
    ############################################
    tokenizer=tokenizer,
    max_length=max_length,
    ################ Modification 3 ###########
    map_fn=oasst1_map_fn,
    ############################################
    pack_to_max_length=True)

train_dataloader = dict(
    batch_size=batch_size,
    num_workers=dataloader_num_workers,
    dataset=train_dataset,
    sampler=dict(type=DefaultSampler, shuffle=True),
    collate_fn=dict(type=default_collate_fn))
#######################################################################
#                            STEP 4  Scheduler                        #
#######################################################################
...
#######################################################################
#                           STEP 5  Runtime                           #
#######################################################################
...
```

éœ€è¦è¿›è¡Œä»¥ä¸‹ä¸‰ç‚¹ä¿®æ”¹ï¼š

- å®ç°map_fnå°†åŸå§‹æ•°æ®é›†æ˜ å°„ä¸ºxtuneræ ‡å‡†æ•°æ®é›†æ ¼å¼ï¼Œå¹¶**åœ¨configä¸­importè¿›æ¥**ï¼ˆå¯¹åº”Modification 1ï¼‰
- ç”¨importè¿›æ¥çš„map_fnæ›¿æ¢æ‰`train_dataset`ä¸­çš„map_fnï¼ˆå¯¹åº”Modification 3ï¼‰
- ä¿®æ”¹åŸå§‹æ•°æ®é›†è·¯å¾„ï¼ˆå¯¹åº”Modification 2ï¼‰ï¼Œload_datasetç›¸å…³æ“ä½œå¯ä»¥å‚è€ƒ[ç”¨æˆ·æ–‡æ¡£](https://huggingface.co/docs/datasets/loading)

ä¸‹é¢ä»‹ç»å¦‚ä½•å®ç°æ•°æ®é›†å¯¹åº”çš„map_fnã€‚

ç”±äºä¸åŒæ•°æ®é›†çš„æ ¼å¼å„æœ‰ä¸åŒï¼Œå› æ­¤éœ€è¦å°†åŸå§‹æ•°æ®æ˜ å°„ä¸ºxTunerå®šä¹‰çš„[å¢é‡é¢„è®­ç»ƒæ•°æ®æ ¼å¼](./dataset_format.md##å¢é‡é¢„è®­ç»ƒæ•°æ®é›†æ ¼å¼)ã€‚xTuneræ”¯æŒé€šè¿‡map functionæ¥å®ç°æ ¼å¼çš„æ˜ å°„ã€‚ä¸‹é¢ä»¥[oasst1](https://huggingface.co/datasets/OpenAssistant/oasst1)æ•°æ®é›†ä¸ºä¾‹ä»‹ç»å¦‚ä½•å®ç°æ•°æ®æ˜ å°„ã€‚

oasst1æ•°æ®é›†æ ¼å¼å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
>>> from datasets import load_dataset

>>> ds = load_dataset(path='timdettmers/openassistant-guanaco')
>>> ds['train']
Dataset({
    features: ['text'],
    num_rows: 9846
})
```

ç”±æ­¤å¯è§ï¼Œoasst1 train datasetæœ‰9846è¡Œï¼Œ1åˆ—ï¼Œåˆ—åä¸º'text'ï¼Œ'text'è¿™ä¸€åˆ—æ­£æ˜¯å¢é‡é¢„è®­ç»ƒéœ€è¦ç”¨åˆ°çš„æ–‡æœ¬æ•°æ®ã€‚[å¢é‡é¢„è®­ç»ƒæ•°æ®æ ¼å¼](./dataset_format.md##å¢é‡é¢„è®­ç»ƒæ•°æ®é›†æ ¼å¼)ä¸­ä»‹ç»äº†å¢é‡é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ•°æ®æ ¼å¼åº”è¯¥ä¸ºï¼š

```json
[{
    "conversation":[
        {
            "input": "",
            "output": "xxx"
        },
    ]
}]
```

å› æ­¤ï¼Œå¯ä»¥é€šè¿‡ä¸‹é¢çš„map functionå°†åŸå§‹æ•°æ®æ˜ å°„ä¸ºæ ‡å‡†æ ¼å¼ï¼š

```python
>>> def oasst1_map_fn(example):
        """
        >>> train_ds = ds['train'].map(oasst1_map_fn)
        >>> train_ds
        Dataset({
            features: ['text', 'conversation'],
            num_rows: 9846
        })
        >>> train_ds[0]['conversation']
        [{'input': '', 'output': 'xxx'}]
        """
        return {'conversation': [{'input': '', 'output': example['text']}]}

```

### ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†

#### Step 1

æŒ‰ç…§xTunerå®šä¹‰çš„[å¢é‡é¢„è®­ç»ƒæ•°æ®æ ¼å¼](./dataset_format.md##å¢é‡é¢„è®­ç»ƒæ•°æ®é›†æ ¼å¼)å‡†å¤‡è‡ªå®šä¹‰æ•°æ®ï¼š

```json
[
    {
        "conversation":[
            {
                "input": "",
                "output": "xxx"
            },
        ]
    },
    {
        "conversation":[
            {
                "input": "",
                "output": "xxx"
            },
        ]
    },
    // ...
]
```

#### Step 2

#### Step1 åˆ—å‡ºå€™é€‰æ¨¡å‹åå­—

```bash
xtuner list-cfg -p internlm
```

`-p`ä¸ºæ¨¡ç³ŠæŸ¥æ‰¾ï¼Œè‹¥æƒ³è®­ç»ƒå…¶ä»–æ¨¡å‹ï¼Œå¯ä»¥ä¿®æ”¹`internlm`ä¸ºxtuneræ”¯æŒçš„å…¶ä»–æ¨¡å‹åç§°ã€‚

#### Step2 å¤åˆ¶configæ–‡ä»¶

```bash
xtuner copy-cfg internlm_7b_qlora_oasst1_e3 xtuner/configs/internlm/internlm_7b/
```

#### Step 3 ä¿®æ”¹configæ–‡ä»¶

ä¿®æ”¹step2å¤åˆ¶å¾—åˆ°çš„configæ–‡ä»¶ä¸­çš„åŸå§‹æ•°æ®é›†è·¯å¾„ï¼ˆå¯¹åº”Modification 1ï¼‰å³å¯ï¼š

```python
from xtuner.datasets import process_hf_dataset
from datasets import load_dataset
...
#######################################################################
#                          STEP 1  Settings                           #
#######################################################################
...
#######################################################################
#                      STEP 2  Model & Tokenizer                      #
#######################################################################
...
#######################################################################
#                      STEP 3  Dataset & Dataloader                   #
#######################################################################
train_dataset = dict(
    type=process_hf_dataset,
    ################ Modification 1 ###########
    dataset=dict(type=load_dataset, path=data_path),
    ############################################
    tokenizer=tokenizer,
    max_length=max_length,
    map_fn=None,
    pack_to_max_length=True)

train_dataloader = dict(
    batch_size=batch_size,
    num_workers=dataloader_num_workers,
    dataset=train_dataset,
    sampler=dict(type=DefaultSampler, shuffle=True),
    collate_fn=dict(type=default_collate_fn))
#######################################################################
#                            STEP 4  Scheduler                        #
#######################################################################
...
#######################################################################
#                           STEP 5  Runtime                           #
#######################################################################
...
```
