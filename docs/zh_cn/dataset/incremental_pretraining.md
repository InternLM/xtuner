# å¢é‡é¢„è®­ç»ƒdata pipeline

å¢é‡é¢„è®­ç»ƒæ—¨åœ¨æå‡æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸæˆ–ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå…¶æ•°æ®å¤„ç†æµç¨‹å¯ä»¥åˆ†ä¸ºä»¥ä¸‹ä¸¤éƒ¨åˆ†ï¼š

1. æ‰©å……tokenizerè¯è¡¨å¤§å°ï¼ˆå¯é€‰ï¼‰
2. æŒ‰ç…§ç›¸åº”æ•°æ®é›†æ ¼å¼æ„å»ºæ•°æ®é›†

## æ‰©å……tokenizerè¯è¡¨ï¼ˆå¯é€‰ï¼Œæ­£åœ¨å¼€å‘ä¸­Â·Â·Â·ï¼‰

ğŸ’¡ ä¸ºäº†é€‚åº”è¯è¡¨æ‰©å±•æ‰€å¸¦æ¥çš„æ¨¡å‹æƒé‡ç»´åº¦å˜åŒ–ï¼ŒxTunerä¼šæ ¹æ®å­—è¡¨å¤§å°è‡ªåŠ¨è°ƒæ•´è¯­è¨€æ¨¡å‹çš„token embedding layerå’Œlm headçš„å‚æ•°ç»´åº¦ã€‚ç”±äºè¿™ä¸ªæ“ä½œä¼šå¼•å…¥ä¸€äº›éšæœºåˆå§‹åŒ–çš„å‚æ•°ï¼Œå› æ­¤é€šå¸¸éœ€è¦åœ¨æ›´å¤§çš„è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒä»¥è·å¾—æ›´å¥½çš„æ•ˆæœã€‚

## æ•°æ®é›†æ„å»º

xTuneræ”¯æŒä½¿ç”¨HuggingFace Hubæ•°æ®é›†æˆ–è‡ªå®šä¹‰æ•°æ®é›†è¿›è¡ŒSFTï¼ˆSupervised FineTuneï¼‰ã€‚äºŒè€…çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œä½¿ç”¨HuggingFace Hubæ•°æ®é›†æ—¶éœ€è¦å°†åŸå§‹æ•°æ®æ˜ å°„ä¸ºxTunerå®šä¹‰çš„[å¢é‡é¢„è®­ç»ƒæ•°æ®æ ¼å¼](./dataset_format.md#å¢é‡é¢„è®­ç»ƒæ•°æ®é›†æ ¼å¼)ã€‚è€Œå¯¹äºè‡ªå®šä¹‰æ•°æ®é›†åˆ™æ¨èç”¨æˆ·æŒ‰ç…§[å¢é‡é¢„è®­ç»ƒæ•°æ®æ ¼å¼](./dataset_format.md#å¢é‡é¢„è®­ç»ƒæ•°æ®é›†æ ¼å¼)æ„é€ æ•°æ®é›†ã€‚

### ä½¿ç”¨HuggingFace Hubæ•°æ®é›†

#### Step 1 æ˜ å°„åŸå§‹æ•°æ®é›†ä¸ºæ ‡å‡†æ ¼å¼

ç”±äºä¸åŒæ•°æ®é›†çš„æ ¼å¼å„æœ‰ä¸åŒï¼Œå› æ­¤éœ€è¦å°†åŸå§‹æ•°æ®æ˜ å°„ä¸ºxTunerå®šä¹‰çš„[å¢é‡é¢„è®­ç»ƒæ•°æ®æ ¼å¼](./dataset_format.md#å¢é‡é¢„è®­ç»ƒæ•°æ®é›†æ ¼å¼)ã€‚xTuneræ”¯æŒé€šè¿‡map functionæ¥å®ç°æ ¼å¼çš„æ˜ å°„ã€‚ä¸‹é¢ä»¥[oasst1](https://huggingface.co/datasets/OpenAssistant/oasst1)æ•°æ®é›†ä¸ºä¾‹ä»‹ç»å¦‚ä½•å®ç°æ•°æ®æ˜ å°„ã€‚

oasst1æ•°æ®é›†æ ¼å¼å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
>>> from datasets import load_dataset

>>> ds = load_dataset(path='timdettmers/openassistant-guanaco')
>>> ds['train']
Dataset({
    features: ['text'],
    num_rows: 9846
})
```

ç”±æ­¤å¯è§ï¼Œoasst1 train datasetæœ‰9846è¡Œï¼Œ1åˆ—ï¼Œåˆ—åä¸º'text'ï¼Œ'text'è¿™ä¸€åˆ—æ­£æ˜¯å¢é‡é¢„è®­ç»ƒéœ€è¦ç”¨åˆ°çš„æ–‡æœ¬æ•°æ®ã€‚[å¢é‡é¢„è®­ç»ƒæ•°æ®æ ¼å¼](./dataset_format.md#å¢é‡é¢„è®­ç»ƒæ•°æ®é›†æ ¼å¼)ä¸­ä»‹ç»äº†å¢é‡é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ•°æ®æ ¼å¼åº”è¯¥ä¸ºï¼š

```json
[{
    "conversation":[
        {
            "input": "",
            "output": "xxx"
        },
    ]
}]
```

å› æ­¤ï¼Œå¯ä»¥é€šè¿‡ä¸‹é¢çš„map functionå°†åŸå§‹æ•°æ®æ˜ å°„ä¸ºæ ‡å‡†æ ¼å¼ï¼š

```python
# å‡è®¾å°†è¯¥å‡½æ•°å­˜æ”¾åœ¨./map_fn.pyæ–‡ä»¶ä¸­
def oasst1_incremental_map_fn(example):
    """
    >>> train_ds = ds['train'].map(oasst1_map_fn)
    >>> train_ds
    Dataset({
        features: ['text', 'conversation'],
        num_rows: 9846
    })
    >>> train_ds[0]['conversation']
    [{'input': '', 'output': 'xxx'}]
    """
    return {'conversation': [{'input': '', 'output': example['text']}]}

```

#### Step 2 åˆ—å‡ºå€™é€‰æ¨¡å‹åå­—

xTuner æä¾›å¤šä¸ªå¼€ç®±å³ç”¨çš„é…ç½®æ–‡ä»¶ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä¸‹åˆ—å‘½ä»¤æŸ¥çœ‹ï¼š

```bash
xtuner list-cfg -p internlm
```

`-p`ä¸ºæ¨¡ç³ŠæŸ¥æ‰¾ï¼Œè‹¥æƒ³è®­ç»ƒå…¶ä»–æ¨¡å‹ï¼Œå¯ä»¥ä¿®æ”¹`internlm`ä¸ºxtuneræ”¯æŒçš„å…¶ä»–æ¨¡å‹åç§°ã€‚

#### Step 3 å¯¼å‡ºconfigæ–‡ä»¶

å¦‚æœæ‰€æä¾›çš„é…ç½®æ–‡ä»¶ä¸èƒ½æ»¡è¶³ä½¿ç”¨éœ€æ±‚ï¼Œè¯·å¯¼å‡ºæ‰€æä¾›çš„é…ç½®æ–‡ä»¶å¹¶è¿›è¡Œç›¸åº”æ›´æ”¹ï¼š

```bash
xtuner copy-cfg ${CONFIG_NAME} ${SAVE_DIR}
```

ä¾‹å¦‚é€šè¿‡ä¸‹åˆ—å‘½ä»¤å°†åä¸º`internlm_7b_qlora_oasst1_e3`çš„configå¯¼å‡ºè‡³å½“å‰ç›®å½•ä¸‹ï¼š

```bash
xtuner copy-cfg internlm_7b_qlora_oasst1_e3 .
```

#### Step 4 ä¿®æ”¹configæ–‡ä»¶

å¯¹step 3å¤åˆ¶å¾—åˆ°çš„configæ–‡ä»¶éœ€è¦è¿›è¡Œå¦‚ä¸‹ä¿®æ”¹ï¼š

1. import Step 1 ä¸­å®ç°çš„æ˜ å°„å‡½æ•° `oasst1_incremental_map_fn`
2. ç”¨`oasst1_incremental_map_fn`æ›¿æ¢`train_dataset`ä¸­çš„`dataset_map_fn`
3. å°†`train_dataset`ä¸­çš„`template_map_fn`ç½®ä¸ºNoneï¼Œå› ä¸ºä¸éœ€è¦å°†å¯¹è¯æ¨¡æ¿åŠ å…¥è‡³å¢é‡é¢„è®­ç»ƒæ•°æ®é›†ä¸­
4. è°ƒæ•´åŸå§‹æ•°æ®é›†çš„è·¯å¾„ï¼Œå…³äº`load_dataset`çš„ç›¸å…³æ“ä½œå¯ä»¥å‚è€ƒ[ç”¨æˆ·æ–‡æ¡£](https://huggingface.co/docs/datasets/loading)
5. ï¼ˆå¯é€‰ï¼‰å¦‚æœä½ å¸Œæœ›åˆ©ç”¨ xTuner æä¾›çš„`EvaluateChatHook`åœ¨è®­ç»ƒçš„å„ä¸ªé˜¶æ®µè®°å½•æ¨¡å‹çš„å¯¹è¯ç»“æœï¼Œä½ è¿˜éœ€è¦é€šè¿‡`prompt_template = PROMPT_TEMPLATE.openassistant`å°†å¯¹è¯æ¨¡æ¿è°ƒæ•´ä¸ºä¸`oasst1`æ•°æ®é›†å¯¹åº”çš„å¯¹è¯æ¨¡æ¿ã€‚xTuneræä¾›äº†ä¸€ç³»åˆ—å¯¹è¯æ¨¡æ¿ï¼Œä½ å¯ä»¥åœ¨`xtuner/utils/templates.py`ä¸­æ‰¾åˆ°ã€‚å…¶ä¸­ï¼Œ`INSTRUCTION_START`å’Œ`INSTRUCTION`åˆ†åˆ«ä»£è¡¨ç¬¬ä¸€è½®å¯¹è¯å’Œåç»­è‹¥å¹²è½®å¯¹è¯æ‰€ä½¿ç”¨çš„å¯¹è¯æ¨¡æ¿ã€‚åœ¨`EvaluateChatHook`ä¸­åªä¼šç”¨åˆ°`INSTRUCTION_START`ã€‚ï¼ˆæ³¨æ„ï¼šç”±äºå¢é‡é¢„è®­ç»ƒåçš„æ¨¡å‹åªå…·å¤‡ç»­å†™åŠŸèƒ½ï¼Œä¸å…·å¤‡å¯¹è¯åŠŸèƒ½ï¼Œå› æ­¤åœ¨ `EvaluateChatHook`æ‰“å°çš„å¯¹è¯ç»“æœä¸­ï¼Œæ¨¡å‹æ— æ³•æ­£å¸¸åœæ­¢ç”Ÿæˆæ˜¯æ­£å¸¸ç°è±¡ã€‚ï¼‰

```diff
from xtuner.datasets import process_hf_dataset
from datasets import load_dataset
- from xtuner.datasets.map_fns import oasst1_map_fn, template_map_fn_factory
+ from xtuner.datasets.map_fns import template_map_fn_factory
+ from map_fn import oasst1_incremental_map_fn
...
#######################################################################
#                          PART 1  Settings                           #
#######################################################################
- data_path = 'timdettmers/openassistant-guanaco'
+ data_path = 'path/to/your/data'

+ prompt_template = PROMPT_TEMPLATE.openassistant
#######################################################################
#                      STEP 3  Dataset & Dataloader                   #
#######################################################################
train_dataset = dict(
    type=process_hf_dataset,
    dataset=dict(type=load_dataset, path=data_path),
    tokenizer=tokenizer,
    max_length=max_length,
-   dataset_map_fn=oasst1_map_fn,
+   dataset_map_fn=oasst1_incremental_map_fn,
-   template_map_fn=dict(
-       type=template_map_fn_factory, template=prompt_template),
+   template_map_fn=None,
    remove_unused_columns=True,
    shuffle_before_pack=True,
    pack_to_max_length=True)

train_dataloader = dict(
    batch_size=batch_size,
    num_workers=dataloader_num_workers,
    dataset=train_dataset,
    sampler=dict(type=DefaultSampler, shuffle=True),
    collate_fn=dict(type=default_collate_fn))
...
```

### ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†

åœ¨ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†è¿›è¡Œå¢é‡é¢„è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬æ¨èå°†æ•°æ®é›†æ„é€ ä¸ºxTunerå®šä¹‰çš„[å¢é‡é¢„è®­ç»ƒæ•°æ®æ ¼å¼](./dataset_format.md#å¢é‡é¢„è®­ç»ƒæ•°æ®é›†æ ¼å¼)ã€‚è‹¥è‡ªå®šä¹‰æ•°æ®é›†æ ¼å¼ä¸º`oasst1`ç­‰å…¶ä»–æ ¼å¼ï¼Œå¯å‚è€ƒ[ä½¿ç”¨HuggingFace Hubæ•°æ®é›†](#ä½¿ç”¨huggingface-hubæ•°æ®é›†)ä¸€èŠ‚ã€‚

#### Step 1 æ•°æ®å‡†å¤‡

æŒ‰ç…§xTunerå®šä¹‰çš„[å¢é‡é¢„è®­ç»ƒæ•°æ®æ ¼å¼](./dataset_format.md#å¢é‡é¢„è®­ç»ƒæ•°æ®é›†æ ¼å¼)å‡†å¤‡è‡ªå®šä¹‰æ•°æ®ï¼š

```json
[
    {
        "conversation":[
            {
                "input": "",
                "output": "xxx"
            },
        ]
    },
    {
        "conversation":[
            {
                "input": "",
                "output": "xxx"
            },
        ]
    },
]
```

#### Step 2 åˆ—å‡ºå€™é€‰æ¨¡å‹åå­—

```bash
xtuner list-cfg -p internlm
```

`-p`ä¸ºæ¨¡ç³ŠæŸ¥æ‰¾ï¼Œè‹¥æƒ³è®­ç»ƒå…¶ä»–æ¨¡å‹ï¼Œå¯ä»¥ä¿®æ”¹`internlm`ä¸ºxtuneræ”¯æŒçš„å…¶ä»–æ¨¡å‹åç§°ã€‚

#### Step 3 å¤åˆ¶configæ–‡ä»¶

```bash
xtuner copy-cfg internlm_7b_qlora_oasst1_e3 .
```

#### Step 4 ä¿®æ”¹configæ–‡ä»¶

å¯¹step 3å¤åˆ¶å¾—åˆ°çš„configæ–‡ä»¶éœ€è¦è¿›è¡Œå¦‚ä¸‹ä¿®æ”¹ï¼š

1. è°ƒæ•´åŸå§‹æ•°æ®é›†çš„è·¯å¾„
2. ç”±äºæ•°æ®é›†æ ¼å¼å·²ç»æ˜¯æ ‡å‡†æ ¼å¼äº†ï¼Œéœ€è¦å°†`train_dataset`ä¸­çš„`dataset_map_fn`ç½®ä¸ºNone
3. å°†`train_dataset`ä¸­çš„`template_map_fn`ç½®ä¸ºNoneï¼Œå› ä¸ºä¸éœ€è¦å°†å¯¹è¯æ¨¡æ¿åŠ å…¥è‡³å¢é‡é¢„è®­ç»ƒæ•°æ®é›†ä¸­
4. ï¼ˆå¯é€‰ï¼‰è®¾ç½®å¯¹è¯æ¨¡æ¿ä»¥è°ƒç”¨`EvaluateChatHook`åœ¨è®­ç»ƒçš„å„ä¸ªé˜¶æ®µè®°å½•æ¨¡å‹çš„å¯¹è¯ç»“æœ

```diff
from xtuner.datasets import process_hf_dataset
from datasets import load_dataset
- from xtuner.datasets.map_fns import oasst1_map_fn, template_map_fn_factory
+ from xtuner.datasets.map_fns import template_map_fn_factory
...
#######################################################################
#                          PART 1  Settings                           #
#######################################################################
- data_path = 'timdettmers/openassistant-guanaco'
+ data_path = 'path/to/your/data'

+ prompt_template = PROMPT_TEMPLATE.openassistant
...
#######################################################################
#                      STEP 3  Dataset & Dataloader                   #
#######################################################################
train_dataset = dict(
    type=process_hf_dataset,
    dataset=dict(type=load_dataset, path=data_path),
    tokenizer=tokenizer,
    max_length=max_length,
-   dataset_map_fn=oasst1_map_fn,
+   dataset_map_fn=oasst1_incremental_map_fn,
-   template_map_fn=dict(
-       type=template_map_fn_factory, template=prompt_template),
+   template_map_fn=None,
    remove_unused_columns=True,
    shuffle_before_pack=True,
    pack_to_max_length=pack_to_max_length)

train_dataloader = dict(
    batch_size=batch_size,
    num_workers=dataloader_num_workers,
    dataset=train_dataset,
    sampler=dict(type=DefaultSampler, shuffle=True),
    collate_fn=dict(type=default_collate_fn))
...
```
