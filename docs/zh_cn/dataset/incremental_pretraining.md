# å¢é‡é¢„è®­ç»ƒdata pipeline

å¢é‡é¢„è®­ç»ƒæ—¨åœ¨æå‡æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸæˆ–ä»»åŠ¡çš„èƒ½åŠ›ï¼Œå…¶æ•°æ®å¤„ç†æµç¨‹å¯ä»¥åˆ†ä¸ºä»¥ä¸‹ä¸¤éƒ¨åˆ†ï¼š

1. æ‰©å……tokenizerè¯è¡¨å¤§å°ï¼ˆå¯é€‰ï¼‰
2. æŒ‰ç…§ç›¸åº”æ•°æ®é›†æ ¼å¼æ„å»ºæ•°æ®é›†

## æ‰©å……tokenizerè¯è¡¨ï¼ˆå¯é€‰ï¼Œæ­£åœ¨å¼€å‘ä¸­Â·Â·Â·ï¼‰

ğŸ’¡ ä¸ºäº†é€‚åº”è¯è¡¨æ‰©å±•æ‰€å¸¦æ¥çš„æ¨¡å‹æƒé‡ç»´åº¦å˜åŒ–ï¼ŒxTunerä¼šæ ¹æ®å­—è¡¨å¤§å°è‡ªåŠ¨è°ƒæ•´è¯­è¨€æ¨¡å‹çš„token embedding layerå’Œlm headçš„å‚æ•°ç»´åº¦ã€‚ç”±äºè¿™ä¸ªæ“ä½œä¼šå¼•å…¥ä¸€äº›éšæœºåˆå§‹åŒ–çš„å‚æ•°ï¼Œå› æ­¤é€šå¸¸éœ€è¦åœ¨æ›´å¤§çš„è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒä»¥è·å¾—æ›´å¥½çš„æ•ˆæœã€‚

## æ•°æ®é›†æ„å»º

xTuneræ”¯æŒä½¿ç”¨HuggingFace Hubæ•°æ®é›†æˆ–è‡ªå®šä¹‰æ•°æ®é›†è¿›è¡ŒSFTï¼ˆSupervised FineTuneï¼‰ã€‚äºŒè€…çš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œä½¿ç”¨HuggingFace Hubæ•°æ®é›†æ—¶éœ€è¦å°†åŸå§‹æ•°æ®æ˜ å°„ä¸ºxTunerå®šä¹‰çš„[å¢é‡é¢„è®­ç»ƒæ•°æ®æ ¼å¼](./dataset_format.md#å¢é‡é¢„è®­ç»ƒæ•°æ®é›†æ ¼å¼)ï¼Œè€Œå¯¹äºè‡ªå®šä¹‰æ•°æ®é›†åˆ™éœ€è¦ç”¨æˆ·æŒ‰ç…§[å¢é‡é¢„è®­ç»ƒæ•°æ®æ ¼å¼](./dataset_format.md#å¢é‡é¢„è®­ç»ƒæ•°æ®é›†æ ¼å¼)æ„é€ æ•°æ®é›†ã€‚

### ä½¿ç”¨HuggingFace Hubæ•°æ®é›†

#### Step 1 æ˜ å°„åŸå§‹æ•°æ®é›†ä¸ºæ ‡å‡†æ ¼å¼

ç”±äºä¸åŒæ•°æ®é›†çš„æ ¼å¼å„æœ‰ä¸åŒï¼Œå› æ­¤éœ€è¦å°†åŸå§‹æ•°æ®æ˜ å°„ä¸ºxTunerå®šä¹‰çš„[å¢é‡é¢„è®­ç»ƒæ•°æ®æ ¼å¼](./dataset_format.md#å¢é‡é¢„è®­ç»ƒæ•°æ®é›†æ ¼å¼)ã€‚xTuneræ”¯æŒé€šè¿‡map functionæ¥å®ç°æ ¼å¼çš„æ˜ å°„ã€‚ä¸‹é¢ä»¥[oasst1](https://huggingface.co/datasets/OpenAssistant/oasst1)æ•°æ®é›†ä¸ºä¾‹ä»‹ç»å¦‚ä½•å®ç°æ•°æ®æ˜ å°„ã€‚

oasst1æ•°æ®é›†æ ¼å¼å¦‚ä¸‹æ‰€ç¤ºï¼š

```python
>>> from datasets import load_dataset

>>> ds = load_dataset(path='timdettmers/openassistant-guanaco')
>>> ds['train']
Dataset({
    features: ['text'],
    num_rows: 9846
})
```

ç”±æ­¤å¯è§ï¼Œoasst1 train datasetæœ‰9846è¡Œï¼Œ1åˆ—ï¼Œåˆ—åä¸º'text'ï¼Œ'text'è¿™ä¸€åˆ—æ­£æ˜¯å¢é‡é¢„è®­ç»ƒéœ€è¦ç”¨åˆ°çš„æ–‡æœ¬æ•°æ®ã€‚[å¢é‡é¢„è®­ç»ƒæ•°æ®æ ¼å¼](./dataset_format.md#å¢é‡é¢„è®­ç»ƒæ•°æ®é›†æ ¼å¼)ä¸­ä»‹ç»äº†å¢é‡é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ•°æ®æ ¼å¼åº”è¯¥ä¸ºï¼š

```json
[{
    "conversation":[
        {
            "input": "",
            "output": "xxx"
        },
    ]
}]
```

å› æ­¤ï¼Œå¯ä»¥é€šè¿‡ä¸‹é¢çš„map functionå°†åŸå§‹æ•°æ®æ˜ å°„ä¸ºæ ‡å‡†æ ¼å¼ï¼š

```python
>>> def oasst1_map_fn(example):
        """
        >>> train_ds = ds['train'].map(oasst1_map_fn)
        >>> train_ds
        Dataset({
            features: ['text', 'conversation'],
            num_rows: 9846
        })
        >>> train_ds[0]['conversation']
        [{'input': '', 'output': 'xxx'}]
        """
        return {'conversation': [{'input': '', 'output': example['text']}]}

```

#### Step 2 åˆ—å‡ºå€™é€‰æ¨¡å‹åå­—

```bash
xtuner list-cfg -p internlm
```

`-p`ä¸ºæ¨¡ç³ŠæŸ¥æ‰¾ï¼Œè‹¥æƒ³è®­ç»ƒå…¶ä»–æ¨¡å‹ï¼Œå¯ä»¥ä¿®æ”¹`internlm`ä¸ºxtuneræ”¯æŒçš„å…¶ä»–æ¨¡å‹åç§°ã€‚

#### Step 3 å¤åˆ¶configæ–‡ä»¶

```bash
xtuner copy-cfg internlm_7b_qlora_oasst1_e3 xtuner/configs/internlm/internlm_7b/
```

#### Step 4 ä¿®æ”¹configæ–‡ä»¶

å¯¹step 3å¤åˆ¶å¾—åˆ°çš„configæ–‡ä»¶éœ€è¦è¿›è¡Œå¦‚ä¸‹ä¿®æ”¹ï¼š

1. import Step 1 ä¸­å®ç°çš„map function `oasst1_map_fn`
2. ç”¨`oasst1_map_fn`æ›¿æ¢`train_dataset`ä¸­çš„map_fn
3. ä¿®æ”¹åŸå§‹æ•°æ®é›†è·¯å¾„ï¼Œload_datasetç›¸å…³æ“ä½œå¯ä»¥å‚è€ƒ[ç”¨æˆ·æ–‡æ¡£](https://huggingface.co/docs/datasets/loading)

```diff
from xtuner.datasets import process_hf_dataset
from datasets import load_dataset
+ from xtuner.datasets.map_fns import oasst1_map_fn
...
#######################################################################
#                      STEP 3  Dataset & Dataloader                   #
#######################################################################
train_dataset = dict(
    type=process_hf_dataset,
-   dataset=dict(type=load_dataset, path=data_path),
+   dataset=dict(type=load_dataset, path='path/to/your/data'),
    tokenizer=tokenizer,
    max_length=max_length,
+   map_fn=oasst1_map_fn,
    ############################################
    pack_to_max_length=True)

train_dataloader = dict(
    batch_size=batch_size,
    num_workers=dataloader_num_workers,
    dataset=train_dataset,
    sampler=dict(type=DefaultSampler, shuffle=True),
    collate_fn=dict(type=default_collate_fn))
...
```

### ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†

#### Step 1 æ•°æ®å‡†å¤‡

æŒ‰ç…§xTunerå®šä¹‰çš„[å¢é‡é¢„è®­ç»ƒæ•°æ®æ ¼å¼](./dataset_format.md#å¢é‡é¢„è®­ç»ƒæ•°æ®é›†æ ¼å¼)å‡†å¤‡è‡ªå®šä¹‰æ•°æ®ï¼š

```json
[
    {
        "conversation":[
            {
                "input": "",
                "output": "xxx"
            },
        ]
    },
    {
        "conversation":[
            {
                "input": "",
                "output": "xxx"
            },
        ]
    },
]
```

#### Step 2 åˆ—å‡ºå€™é€‰æ¨¡å‹åå­—

```bash
xtuner list-cfg -p internlm
```

`-p`ä¸ºæ¨¡ç³ŠæŸ¥æ‰¾ï¼Œè‹¥æƒ³è®­ç»ƒå…¶ä»–æ¨¡å‹ï¼Œå¯ä»¥ä¿®æ”¹`internlm`ä¸ºxtuneræ”¯æŒçš„å…¶ä»–æ¨¡å‹åç§°ã€‚

#### Step 3 å¤åˆ¶configæ–‡ä»¶

```bash
xtuner copy-cfg internlm_7b_qlora_oasst1_e3 xtuner/configs/internlm/internlm_7b/
```

#### Step 4 ä¿®æ”¹configæ–‡ä»¶

ä¿®æ”¹step 3å¤åˆ¶å¾—åˆ°çš„configæ–‡ä»¶ä¸­çš„åŸå§‹æ•°æ®é›†è·¯å¾„å³å¯ï¼š

```diff
from xtuner.datasets import process_hf_dataset
from datasets import load_dataset
...
#######################################################################
#                      STEP 3  Dataset & Dataloader                   #
#######################################################################
train_dataset = dict(
    type=process_hf_dataset,
-   dataset=dict(type=load_dataset, path=data_path),
+   dataset=dict(type=load_dataset, path='path/to/your/data'),
    tokenizer=tokenizer,
    max_length=max_length,
    map_fn=None,
    pack_to_max_length=True)

train_dataloader = dict(
    batch_size=batch_size,
    num_workers=dataloader_num_workers,
    dataset=train_dataset,
    sampler=dict(type=DefaultSampler, shuffle=True),
    collate_fn=dict(type=default_collate_fn))
...
```
