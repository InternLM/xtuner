===============================================
Varlen Attention
===============================================

\ :ref:`æ•°æ®é›†æ‹¼æ¥ <pack_to_max_length>` \  ä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†â€œæ•°æ®é›†æ‹¼æ¥â€ç­–ç•¥å¯¹æ¨¡å‹è®­ç»ƒæ•ˆç‡çš„æ˜¾è‘—æå‡ã€‚
ç†è®ºä¸Šï¼Œæ•°æ®é›†æ‹¼æ¥å¯èƒ½ä¼šå¯¹æ³¨æ„åŠ›ï¼ˆAttentionï¼‰æœºåˆ¶çš„è®¡ç®—è¿‡ç¨‹äº§ç”Ÿå½±å“ã€‚è¿™æ˜¯å› ä¸ºï¼Œåœ¨æœªé‡‡ç”¨æ•°æ®æ‹¼æ¥ç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œ
æ¯æ¡æ•°æ®åœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶ä»…ä¸è‡ªèº«ç›¸å…³è”ã€‚ç„¶è€Œï¼Œå½“é‡‡ç”¨æ•°æ®æ‹¼æ¥ç­–ç•¥åï¼Œç”±å¤šæ¡çŸ­æ•°æ®æ‹¼æ¥æˆçš„é•¿æ•°æ®åœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶ä¼šç›¸äº’å…³è”ã€‚
ä»¥ä¸€ä¸ªç”±è‹¥å¹²çŸ­æ•°æ®æ‹¼æ¥æˆé•¿åº¦ä¸º 4096 çš„æ•°æ®ä¸ºä¾‹ï¼Œå¦‚æœä¸é‡‡ç”¨å˜é•¿æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨æ³¨æ„åŠ›è®¡ç®—é˜¶æ®µï¼Œæ¯ä¸ª token å°†ä¼šå…³æ³¨å…¨éƒ¨ 4096 ä¸ª tokens ï¼Œå¦‚å›¾å·¦ä¾§æ‰€ç¤ºã€‚

ç›¸åï¼Œåœ¨ä½¿ç”¨å˜é•¿æ³¨æ„åŠ›æœºåˆ¶çš„æƒ…å†µä¸‹ï¼Œæ¯ä¸ª token åœ¨æ³¨æ„åŠ›è®¡ç®—é˜¶æ®µä»…ä¼šå…³æ³¨å…¶æ‰€åœ¨çŸ­æ•°æ®ä¸­çš„æ‰€æœ‰ tokens ï¼Œå¦‚å›¾å³ä¾§æ‰€ç¤ºã€‚å› æ­¤ï¼Œ **å˜é•¿æ³¨æ„åŠ›æœºåˆ¶ç¡®ä¿äº†æ— è®ºæ˜¯å¦é‡‡ç”¨â€œæ•°æ®é›†æ‹¼æ¥â€ç­–ç•¥ï¼Œæ¨¡å‹è®­ç»ƒçš„è¡Œä¸ºä¿æŒä¸€è‡´æ€§ã€‚**

.. raw:: html

    <p align="center">
        <img src="https://github.com/InternLM/InternLM/assets/41630003/7e0c6a02-a970-4bd3-a10b-8341720bf654" alt="XTuner" width="600"/>
        <br />å˜é•¿æ³¨æ„åŠ›è®¡ç®—åŸç†ï¼ˆæ‹·è´è‡ª https://github.com/InternLM/InternEvo/blob/develop/doc/usage.mdï¼‰<br />
    </p>

æ”¯æŒåˆ—è¡¨
=====================

.. note::

    ä½¿ç”¨å˜é•¿æ³¨æ„åŠ›éœ€è¦é¦–å…ˆå®‰è£… `flash attn <https://github.com/Dao-AILab/flash-attention>`_ ï¼ˆ
    å‚è€ƒ `flash attn å®‰è£… <https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features>`_ ï¼‰

.. list-table::
  :widths: 25 50
  :header-rows: 1

  * - æ¨¡å‹
    - Flash Attention æ”¯æŒæƒ…å†µ
  * - baichuan 1/2
    - âŒ
  * - chatglm 2/3
    - âŒ
  * - deepseek
    - âœ…
  * - gemma
    - âŒ
  * - internlm 1/2
    - âœ…
  * - llama 2
    - âœ…
  * - mistral
    - âœ…
  * - qwen 1/1.5
    - âŒ
  * - starcoder
    - âŒ
  * - yi
    - âœ…
  * - zephyr
    - âœ…

ä½¿ç”¨å˜é•¿æ³¨æ„åŠ›æœºåˆ¶è®­ç»ƒ
=========================

æ­¥éª¤ 1ï¼šå®‰è£… flash_attn
--------------------------

XTuner ä¸­å®ç°çš„å˜é•¿æ³¨æ„åŠ›éœ€è¦ä¾èµ– Flash Attention 2ï¼Œå¯é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼ˆéœ€è¦ cudaï¼‰ï¼š

.. code:: console

  $ MAX_JOBS=4 pip install flash-attn --no-build-isolation

.. tip::
  æ›´å¤šå®‰è£…æ–¹å¼è¯·å‚è€ƒ `flash attn å®‰è£… <https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features>`_

æ­¥éª¤ 2ï¼šæŸ¥æ‰¾æ¨¡æ¿ config
---------------------------

XTuner æä¾›å¤šä¸ªå¼€ç®±å³ç”¨çš„é…ç½®æ–‡ä»¶ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä¸‹åˆ—å‘½ä»¤æŸ¥çœ‹ï¼š

.. code-block:: console

    $ xtuner list-cfg -p internlm

.. tip::
  ``-p`` ä¸ºæ¨¡ç³ŠæŸ¥æ‰¾ï¼Œè‹¥æƒ³è®­ç»ƒå…¶ä»–æ¨¡å‹ï¼Œå¯ä»¥ä¿®æ”¹ ``internlm`` ä¸º XTuner æ”¯æŒçš„å…¶ä»–æ¨¡å‹åç§°ã€‚

æ­¥éª¤ 3ï¼šå¤åˆ¶ config æ–‡ä»¶
-----------------------------

å¯¼å‡ºéœ€è¦ä½¿ç”¨çš„ config ï¼š

.. code-block:: bash

    xtuner copy-cfg ${CONFIG_NAME} ${SAVE_DIR}

ä¾‹å¦‚é€šè¿‡ä¸‹åˆ—å‘½ä»¤å°†åä¸º ``internlm_7b_full_oasst1_e3`` çš„ config å¯¼å‡ºè‡³å½“å‰ç›®å½•ä¸‹ï¼š

.. code-block:: console

    $ xtuner copy-cfg internlm_7b_full_oasst1_e3 .

.. note::

   å½“å‰ç›®å½•ä¸‹ä¼šå­˜åœ¨ä¸€ä¸ªæ–° config
   ``internlm_7b_full_oasst1_e3_copy.py`` ã€‚

æ­¥éª¤ 4ï¼šä¿®æ”¹ config æ–‡ä»¶
-------------------------------

å°†æ­¥éª¤ 3 å¤åˆ¶å¾—åˆ°çš„ config æ–‡ä»¶ä¸­çš„ ``use_varlen_attn`` å±æ€§ç”± False æ”¹ä¸º True å³å¯æ¿€æ´»å˜é•¿æ³¨æ„åŠ›è®­ç»ƒæœºåˆ¶ï¼š

.. code-block:: diff

    ...
    #######################################################################
    #                          PART 1  Settings                           #
    #######################################################################
    # Model
    pretrained_model_name_or_path = 'internlm/internlm-7b'
    - use_varlen_attn = False
    + use_varlen_attn = True
    ...

.. warning::

    å½“è®¾ç½® ``use_varlen_attn = True`` åï¼Œ ``batch_size = 2, max_length = 2k`` çš„é…ç½®ä¸ ``batch_size = 1, max_length = 4k`` çš„é…ç½®è®­ç»ƒè¡Œä¸ºæ˜¯è¿‘ä¼¼çš„ï¼Œ
    å› æ­¤ XTuner ç›®å‰åªæ”¯æŒäº† ``batch_size = 1`` çš„æƒ…å†µã€‚å¦å¤–ï¼Œ ``use_varlen_attn = True`` æ—¶ ``pack_to_max_length`` ä¹Ÿéœ€è®¾ç½®ä¸º Trueã€‚

æ­¥éª¤ 5ï¼šå¼€å§‹è®­ç»ƒ
-----------------------

.. code-block:: bash

    xtuner train ${CONFIG_NAME_OR_PATH}

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥åŸºäºæ­¥éª¤ 4 ä¸­ä¿®æ”¹å¾—åˆ°çš„ `internlm_7b_full_oasst1_e3_copy.py` è¿›è¡Œè®­ç»ƒï¼š

.. code-block:: console

    $ # On a single GPU
    $ xtuner train internlm_7b_full_oasst1_e3_copy.py --deepspeed deepspeed_zero1
    $ # On multiple GPUs(torchrun)
    $ NPROC_PER_NODE=${GPU_NUM} xtuner train internlm_7b_full_oasst1_e3_copy.py --deepspeed deepspeed_zero1
    $ # On multiple GPUs(slurm)
    $ srun ${SRUN_ARGS} xtuner train internlm_7b_full_oasst1_e3_copy.py --launcher slurm --deepspeed deepspeed_zero1

.. tip::
  ``--deepspeed`` è¡¨ç¤ºä½¿ç”¨ `DeepSpeed <https://github.com/microsoft/DeepSpeed>`_ ğŸš€ æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚è‹¥æœªå®‰è£… DeepSpeed ï¼Œå¯é€šè¿‡ ``pip install deepspeed>=0.12.3`` è¿›è¡Œå®‰è£…ã€‚XTuner å†…ç½®äº†å¤šç§ç­–ç•¥ï¼ŒåŒ…æ‹¬ ZeRO-1ã€ZeRO-2ã€ZeRO-3 ç­‰ã€‚å¦‚æœç”¨æˆ·æœŸæœ›å…³é—­æ­¤åŠŸèƒ½ï¼Œè¯·ç›´æ¥ç§»é™¤æ­¤å‚æ•°ã€‚

æ­¥éª¤ 6ï¼šæ¨¡å‹è½¬æ¢
^^^^^^^^^^^^^^^^^^^^^^^^^^^

å°†ä¿å­˜çš„ PTH æ¨¡å‹ï¼ˆå¦‚æœä½¿ç”¨çš„DeepSpeedï¼Œåˆ™å°†ä¼šæ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼‰è½¬æ¢ä¸º HuggingFace æ¨¡å‹ï¼š

.. code-block:: bash

    xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH} ${SAVE_PATH}

å¯¹åº”ä¸Šé¢çš„ä¾‹å­ï¼Œæ¨¡å‹è½¬æ¢è„šæœ¬ä¸ºï¼š

.. code-block:: bash

    xtuner convert pth_to_hf internlm_7b_full_oasst1_e3_copy.py ${PTH} ${SAVE_PATH}

.. note::
  å…¶ä¸­ ``${PTH}`` ä¸ºè®­ç»ƒæƒé‡ä¿å­˜çš„è·¯å¾„ï¼Œè‹¥è®­ç»ƒæ—¶æœªæŒ‡å®šï¼Œé»˜è®¤ä¿å­˜åœ¨ ``./work_dirs/internlm_7b_full_oasst1_e3_copy`` è·¯å¾„ä¸‹ã€‚
