.. xtuner documentation master file, created by
   sphinx-quickstart on Tue Jan  9 16:33:06 2024.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

.. |checked| unicode:: U+2713
.. |unchecked| unicode:: U+2717

æ¬¢è¿æ¥åˆ° XTuner V1 çš„ä¸­æ–‡æ–‡æ¡£
==================================

.. figure:: ./_static/image/logo.png
  :align: center
  :alt: xtuner
  :class: no-scaled-link

.. raw:: html

   <p style="text-align:center">
   <strong>LLM ä¸€ç«™å¼å·¥å…·ç®±
   </strong>
   </p>

   <p style="text-align:center">
   <script async defer src="https://buttons.github.io/buttons.js"></script>
   <a class="github-button" href="https://github.com/InternLM/xtuner" data-show-count="true" data-size="large" aria-label="Star">Star</a>
   <a class="github-button" href="https://github.com/InternLM/xtuner/subscription" data-icon="octicon-eye" data-size="large" aria-label="Watch">Watch</a>
   <a class="github-button" href="https://github.com/InternLM/xtuner/fork" data-icon="octicon-repo-forked" data-size="large" aria-label="Fork">Fork</a>
   </p>

.. toctree::
   :hidden:
   :maxdepth: 2
   :caption: å¼€å§‹ä½¿ç”¨

   get_started/index.rst

.. toctree::
   :hidden:
   :maxdepth: 3
   :caption: é¢„è®­ç»ƒä¸å¾®è°ƒ

   pretrain_sft/tutorial/index.rst

.. toctree::
   :hidden:
   :maxdepth: 3
   :caption: å¼ºåŒ–å­¦ä¹ 

   rl/tutorial/rl_grpo_trainer.md

.. toctree::
   :hidden:
   :maxdepth: 3
   :caption: è¿›é˜¶æ•™ç¨‹

   pretrain_sft/advanced_tutorial/index.rst
   rl/advanced_tutorial/index.rst

.. toctree::
   :hidden:
   :maxdepth: 3
   :caption: Benchmark

   benchmark/index.rst

.. toctree::
   :hidden:
   :maxdepth: 1
   :caption: æ—§ç‰ˆæ–‡æ¡£

   legacy_index.rst

.. .. toctree::
..    :hidden:
..    :maxdepth: 2
..    :caption: API
..
..    Pretrain & SFT Trainer  <api/trainer>
..    Config <api/config>
..    RL Trainer <api/rl_trainer>
..    RL Config <api/rl_config>
..    Loss Context <api/loss_ctx>
  

XTuner V1 æ˜¯ä¸€ä¸ªä¸“ä¸ºè¶…å¤§è§„æ¨¡ MoE æ¨¡å‹æ‰“é€ çš„æ–°ä¸€ä»£å¤§æ¨¡å‹è®­ç»ƒå¼•æ“ã€‚ä¸ä¼ ç»Ÿ 3D å¹¶è¡Œè®­ç»ƒæ¶æ„ç›¸æ¯”ï¼ŒXTuner V1 é’ˆå¯¹å½“å‰å­¦æœ¯ç•Œä¸»æµçš„ MoE è®­ç»ƒåœºæ™¯è¿›è¡Œäº†æ·±åº¦ä¼˜åŒ–ã€‚

ğŸš€ Speed Benchmark
==================================

.. figure:: ../assets/images/benchmark/benchmark.png
   :align: center
   :width: 90%

æ ¸å¿ƒç‰¹æ€§
==========
**ğŸ“Š Dropless è®­ç»ƒ**

- **çµæ´»æ‰©å±•ï¼Œæ— éœ€å¤æ‚é…ç½®ï¼š** 200B é‡çº§ MoE æ— éœ€ä¸“å®¶å¹¶è¡Œï¼›600B MoE ä»…éœ€èŠ‚ç‚¹å†…ä¸“å®¶å¹¶è¡Œ
- **ä¼˜åŒ–çš„å¹¶è¡Œç­–ç•¥ï¼š** ç›¸æ¯”ä¼ ç»Ÿ 3D å¹¶è¡Œæ–¹æ¡ˆï¼Œä¸“å®¶å¹¶è¡Œç»´åº¦æ›´å°ï¼Œå®ç°æ›´é«˜æ•ˆçš„ Dropless è®­ç»ƒ

**ğŸ“ é•¿åºåˆ—æ”¯æŒ**

- **å†…å­˜é«˜æ•ˆè®¾è®¡ï¼š** é€šè¿‡å…ˆè¿›çš„æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯ç»„åˆï¼Œ200B MoE æ¨¡å‹æ— éœ€åºåˆ—å¹¶è¡Œå³å¯è®­ç»ƒ 64k åºåˆ—é•¿åº¦
- **çµæ´»æ‰©å±•èƒ½åŠ›ï¼š** å…¨é¢æ”¯æŒ DeepSpeed Ulysses åºåˆ—å¹¶è¡Œï¼Œæœ€å¤§åºåˆ—é•¿åº¦å¯çº¿æ€§æ‰©å±•
- **ç¨³å®šå¯é ï¼š** é•¿åºåˆ—è®­ç»ƒæ—¶å¯¹ä¸“å®¶è´Ÿè½½ä¸å‡è¡¡ä¸æ•æ„Ÿï¼Œä¿æŒç¨³å®šæ€§èƒ½

**âš¡ å“è¶Šæ•ˆç‡**

- **è¶…å¤§è§„æ¨¡æ”¯æŒï¼š** æ”¯æŒé«˜è¾¾ 1T å‚æ•°é‡çš„ MoE æ¨¡å‹è®­ç»ƒ
- **çªç ´æ€§èƒ½ç“¶é¢ˆï¼š** é¦–æ¬¡åœ¨ 200B ä»¥ä¸Šè§„æ¨¡çš„ MoE æ¨¡å‹ä¸Šï¼Œå®ç° FSDP è®­ç»ƒååè¶…è¶Šä¼ ç»Ÿ 3D å¹¶è¡Œæ–¹æ¡ˆ
- **ç¡¬ä»¶ä¼˜åŒ–ï¼š** åœ¨ Ascend A3 NPU è¶…èŠ‚ç‚¹ä¸Šï¼Œè®­ç»ƒæ•ˆç‡è¶…è¶Š NVIDIA H800


.. figure:: ../assets/images/benchmark/structure.png
   :align: center
   :width: 90%
   :alt: Performance comparison


ğŸ”¥ Roadmap
==========

XTuner V1 è‡´åŠ›äºæŒç»­æå‡è¶…å¤§è§„æ¨¡ MoE æ¨¡å‹çš„é¢„è®­ç»ƒã€æŒ‡ä»¤å¾®è°ƒå’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒæ•ˆç‡ï¼Œå¹¶é‡ç‚¹ä¼˜åŒ–æ˜‡è…¾ NPU æ”¯æŒã€‚

ğŸš€ è®­ç»ƒå¼•æ“
-----------

æˆ‘ä»¬çš„æ„¿æ™¯æ˜¯å°† XTuner V1 æ‰“é€ æˆé€šç”¨è®­ç»ƒåç«¯ï¼Œæ— ç¼é›†æˆåˆ°æ›´å¹¿æ³›çš„å¼€æºç”Ÿæ€ç³»ç»Ÿä¸­ã€‚

+------------+-----------+----------+-----------+
|   Model    |  GPU(FP8) | GPU(BF16)| NPU(BF16) |
+============+===========+==========+===========+
| Intern S1  |    âœ…     |    âœ…    |    âœ…     |
+------------+-----------+----------+-----------+
| Intern VL  |    âœ…     |    âœ…    |    âœ…     |
+------------+-----------+----------+-----------+
| Qwen3 Dense|    âœ…     |    âœ…    |    âœ…     |
+------------+-----------+----------+-----------+
| Qwen3 MoE  |    âœ…     |    âœ…    |    âœ…     |
+------------+-----------+----------+-----------+
| GPT OSS    |    âœ…     |    âœ…    |    âŒ     |
+------------+-----------+----------+-----------+
| Deepseek V3|    âœ…     |    âœ…    |    âŒ     |
+------------+-----------+----------+-----------+
| KIMI K2    |    âœ…     |    âœ…    |    âŒ     |
+------------+-----------+----------+-----------+


ğŸ§  ç®—æ³•å¥—ä»¶
-----------

ç®—æ³•ç»„ä»¶æ­£åœ¨å¿«é€Ÿè¿­ä»£ä¸­ã€‚æ¬¢è¿ç¤¾åŒºè´¡çŒ® - ä½¿ç”¨ XTuner V1ï¼Œå°†æ‚¨çš„ç®—æ³•æ‰©å±•åˆ°å‰æ‰€æœªæœ‰çš„è§„æ¨¡ï¼

**å·²å®ç°**

- âœ… **å¤šæ¨¡æ€é¢„è®­ç»ƒ** - å…¨é¢æ”¯æŒè§†è§‰è¯­è¨€æ¨¡å‹è®­ç»ƒ
- âœ… **å¤šæ¨¡æ€ç›‘ç£å¾®è°ƒ** - é’ˆå¯¹æŒ‡ä»¤è·Ÿéšä¼˜åŒ–
- âœ… `GRPO <https://arxiv.org/pdf/2402.03300>`_ - Group Relative Policy Optimization

**å³å°†æ¨å‡º**

- ğŸ”„ `MPO <https://arxiv.org/pdf/2411.10442>`_ - Mixed Preference Optimization
- ğŸ”„ `DAPO <https://arxiv.org/pdf/2503.14476>`_ - Dynamic Sampling Policy Optimization
- ğŸ”„ **å¤šè½®æ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ** - é«˜çº§æ™ºèƒ½ä½“è®­ç»ƒèƒ½åŠ›


âš¡ æ¨ç†å¼•æ“é›†æˆ
---------------

ä¸ä¸»æµæ¨ç†æ¡†æ¶æ— ç¼å¯¹æ¥

* |checked| LMDeploy
* |unchecked| vLLM
* |unchecked| SGLang



ğŸ¤ è´¡çŒ®æŒ‡å—
-----------

æˆ‘ä»¬æ„Ÿè°¢æ‰€æœ‰çš„è´¡çŒ®è€…ä¸ºæ”¹è¿›å’Œæå‡ XTuner æ‰€ä½œå‡ºçš„åŠªåŠ›ã€‚è¯·å‚è€ƒ `è´¡çŒ®æŒ‡å— <.github/CONTRIBUTING.md>`_ æ¥äº†è§£å‚ä¸é¡¹ç›®è´¡çŒ®çš„ç›¸å…³æŒ‡å¼•ã€‚

ğŸ™ è‡´è°¢
-----------

XTuner V1 çš„å¼€å‘æ·±å—å¼€æºç¤¾åŒºä¼˜ç§€é¡¹ç›®çš„å¯å‘å’Œæ”¯æŒã€‚æˆ‘ä»¬å‘ä»¥ä¸‹å¼€åˆ›æ€§é¡¹ç›®è‡´ä»¥è¯šæŒšçš„è°¢æ„ï¼š

**è®­ç»ƒå¼•æ“ï¼š**

- [Torchtitan](https://github.com/pytorch/torchtitan) - PyTorch åŸç”Ÿåˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶
- [Deepspeed](https://github.com/deepspeedai/DeepSpeed) - å¾®è½¯æ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“	
- [MindSpeed](https://gitee.com/ascend/MindSpeed) - æ˜‡è…¾é«˜æ€§èƒ½è®­ç»ƒåŠ é€Ÿåº“
- [Megatron](https://github.com/NVIDIA/Megatron-LM) - NVIDIA å¤§è§„æ¨¡ Transformer è®­ç»ƒæ¡†æ¶


**å¼ºåŒ–å­¦ä¹ ï¼š**

XTuner V1 çš„å¼ºåŒ–å­¦ä¹ èƒ½åŠ›å€Ÿé‰´äº†ä»¥ä¸‹é¡¹ç›®çš„ä¼˜ç§€å®è·µå’Œç»éªŒ

- [veRL](https://github.com/volcengine/verl) - Volcano Engine Reinforcement Learning for LLMs	
- [SLIME](https://github.com/THUDM/slime) - THU's scalable RLHF implementation	
- [AReal](https://github.com/inclusionAI/AReaL) - Ant Reasoning Reinforcement Learning for LLMs
- [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF) - An Easy-to-use, Scalable and High-performance RLHF Framework based on Ray

æˆ‘ä»¬è¡·å¿ƒæ„Ÿè°¢è¿™äº›é¡¹ç›®çš„æ‰€æœ‰è´¡çŒ®è€…å’Œç»´æŠ¤è€…ï¼Œæ˜¯ä»–ä»¬æ¨åŠ¨äº†å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒé¢†åŸŸçš„ä¸æ–­è¿›æ­¥ã€‚


ğŸ–Šï¸ å¼•ç”¨
-----------

.. code-block:: bibtex

   @misc{2023xtuner,
       title={XTuner: A Toolkit for Efficiently Fine-tuning LLM},
       author={XTuner Contributors},
       howpublished = {\url{https://github.com/InternLM/xtuner}},
       year={2023}
   }

å¼€æºè®¸å¯è¯
==========

è¯¥é¡¹ç›®é‡‡ç”¨ `Apache License 2.0 å¼€æºè®¸å¯è¯ <LICENSE>`_ã€‚åŒæ—¶ï¼Œè¯·éµå®ˆæ‰€ä½¿ç”¨çš„æ¨¡å‹ä¸æ•°æ®é›†çš„è®¸å¯è¯ã€‚


