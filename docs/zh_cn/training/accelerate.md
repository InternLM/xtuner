# åŠ é€Ÿè®­ç»ƒ

## æ•°æ®é›†æ‹¼æ¥

### ç®€ä»‹

å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„è¾“å…¥è€Œè¨€ï¼Œâ€œæ•°æ®é›†æ‹¼æ¥â€ è¿™ä¸€æ¦‚å¿µæŒ‡çš„æ˜¯å°†å¤šä¸ª token åºåˆ—æ‹¼æ¥æˆä¸€ä¸ªå•ç‹¬çš„è¾“å…¥ã€‚å¤§é‡çš„æ•°æ®é›†éƒ½å­˜åœ¨ä¸€ä¸ªç‰¹ç‚¹ï¼Œå³å…¶é•¿åº¦åˆ†å¸ƒä¸¥é‡åå‘è¾ƒçŸ­çš„åºåˆ—ï¼Œè€Œ Transformers æ¨¡å‹æ¥æ”¶å›ºå®šé•¿åº¦çš„è¾“å…¥ã€‚å› æ­¤ï¼Œåœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œé€šå¸¸éœ€è¦å°†æ¯æ¡æ•°æ® "Pad" è‡³å½“å‰ batch æœ€é•¿åºåˆ—çš„é•¿åº¦ï¼Œè€Œ "Pad Token" å¾€å¾€æ˜¯æŸä¸ªç‰¹å®šçš„æ— æ„ä¹‰çš„ tokenã€‚

å°†å¤šæ¡æ•°æ®æ‰“åŒ…åœ¨ä¸€èµ·å¯ä»¥ä¸å†éœ€è¦ä½¿ç”¨ "Pad Token" è¿›è¡Œæ— æ„ä¹‰çš„å¡«å……ï¼Œå‡å°‘è®¡ç®—èµ„æºçš„æµªè´¹ï¼ŒåŒæ—¶è¿˜å¯ä»¥ä¿æŒæ¨¡å‹ä½œä¸ºå…·æœ‰å›ºå®šå¤§å°è¾“å…¥çš„é™æ€å›¾è¡¨ç¤ºçš„ä¼˜ç‚¹ã€‚

ä¸‹è¡¨å±•ç¤ºäº† InternLM2 7B æ¨¡å‹åœ¨ Alpaca æ•°æ®é›†ä¸Šä½¿ç”¨ä¸åŒæ•°æ®é›†æ‹¼æ¥ç­–ç•¥è¿›è¡Œè®­ç»ƒçš„é€Ÿåº¦å¯¹æ¯”ï¼Œå¦‚è¡¨æ‰€ç¤ºï¼Œâ€œæ•°æ®é›†æ‹¼æ¥â€ä¼šå¤§å¹…åº¦æå‡è®­ç»ƒæ•ˆç‡ï¼š

| æ‹¼æ¥ç­–ç•¥   | æ¯ç§’å¤„ç† token æ•° | åŠ é€Ÿæ¯” |
| ---------- | ----------------- | ------ |
| ä¸ä½¿ç”¨     | 362.9             | -      |
| æ‹¼æ¥è‡³ 2k  | 2677.1            | 7.38x  |
| æ‹¼æ¥è‡³ 4k  | 3124.3            | 8.61x  |
| æ‹¼æ¥è‡³ 8k  | 3173.9            | 8.76x  |
| æ‹¼æ¥è‡³ 16k | 2864.4            | 7.89x  |
| æ‹¼æ¥è‡³ 32k | 2965.4            | 8.17x  |

### åœ¨ XTuner ä¸­ä½¿ç”¨æ•°æ®æ‹¼æ¥

XTuner ä¸­æä¾›çš„ config æ–‡ä»¶ä¸­é»˜è®¤ä½¿ç”¨äº†â€œæ•°æ®é›†æ‹¼æ¥â€è¿™ä¸€åŠŸèƒ½ï¼Œå¯ä»¥é€šè¿‡è®¾ç½® `max_length` å­—æ®µæ¥è°ƒæ•´æ•°æ®æ‹¼æ¥é•¿åº¦ã€‚ä¾‹å¦‚å¯é€šè¿‡ä»¥ä¸‹æ–¹å¼å°†æ‹¼æ¥é•¿åº¦è°ƒæ•´ä¸º 32k ï¼š

```diff
#######################################################################
#                          PART 1  Settings                           #
#######################################################################
- max_length = 2048
+ max_length = 32768
pack_to_max_length = True

#######################################################################
#                      PART 3  Dataset & Dataloader                   #
#######################################################################
train_dataset = dict(
    max_length=max_length,
    pack_to_max_length=pack_to_max_length,
    ...)
```

## ä½¿ç”¨ DeepSpeed åŠ é€Ÿè®­ç»ƒ

[DeepSpeed](https://github.com/microsoft/DeepSpeed) æ˜¯ä¸€ä¸ªå¼€æºçš„æ·±åº¦å­¦ä¹ ä¼˜åŒ–åº“ï¼Œæ—¨åœ¨ç®€åŒ–å¹¶åŠ é€Ÿå¤§è§„æ¨¡æ¨¡å‹çš„è®­ç»ƒã€‚

XTuner æ”¯æŒä¸€é”®å¯åŠ¨ DeepSpeed è¿›è¡Œè®­ç»ƒï¼Œåªéœ€åœ¨å¯åŠ¨å‘½ä»¤åæ’å…¥ `--deepspeed deepspeed_zero2(deepspeed_zero1 or deepspeed_zero3)` å³å¯ï¼š

```shell
xtuner train xxx --deepspeed deepspeed_zero2
```

ä¾‹å¦‚è‹¥æƒ³ä½¿ç”¨ DeepSpeed Zero3 æ˜¾å­˜ä¼˜åŒ–ç®—æ³•è¿è¡Œ QLoRA ç®—æ³•åœ¨ oasst1 æ•°æ®é›†ä¸Šå¾®è°ƒ InternLM2-Chat-7Bï¼Œå¯ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ï¼š

```shell
# å•å¡
xtuner train internlm2_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero3
# å¤šå¡
  (DIST) NPROC_PER_NODE=${GPU_NUM} xtuner train internlm2_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero3
  (SLURM) srun ${SRUN_ARGS} xtuner train internlm2_chat_7b_qlora_oasst1_e3 --launcher slurm --deepspeed deepspeed_zero3
```

## ä½¿ç”¨ Flash Attention åŠ é€Ÿè®­ç»ƒ

Flash Attention (Flash Attention 2) æ˜¯ä¸€ç§ç”¨äºåŠ é€Ÿ Transformer æ¨¡å‹ä¸­ Attention è®¡ç®—ï¼Œå¹¶å‡å°‘å…¶æ˜¾å­˜æ¶ˆè€—çš„ç®—æ³•ã€‚XTuner ä¸­ Flash Attention (Flash Attention 2) çš„æ”¯æŒæƒ…å†µå¦‚ä¸‹è¡¨æ‰€ç¤ºï¼š

|   æ¨¡å‹    |        Flash Attention        |
| :-------: | :---------------------------: |
| baichuan  | :negative_squared_cross_mark: |
|  chatglm  | :negative_squared_cross_mark: |
| deepseek  |      :white_check_mark:       |
|   gemma   | :negative_squared_cross_mark: |
| internlm  |      :white_check_mark:       |
|   llama   |      :white_check_mark:       |
|  mistral  |      :white_check_mark:       |
|   qwen    |      :white_check_mark:       |
| starcoder |      :white_check_mark:       |
|    yi     |      :white_check_mark:       |
|  zephyr   |      :white_check_mark:       |

**XTuner ä¼šæ ¹æ®è¿è¡Œç¯å¢ƒè‡ªåŠ¨æ§åˆ¶ Flash Attention çš„ä½¿ç”¨æƒ…å†µï¼š**

| ç¯å¢ƒ                                                                                                 | Flash Attention ä½¿ç”¨æƒ…å†µ |
| ---------------------------------------------------------------------------------------------------- | ------------------------ |
| å®‰è£… [flash attn](https://github.com/Dao-AILab/flash-attention)                                      | Flash Attention 2        |
| æœªå®‰è£… [flash attn](https://github.com/Dao-AILab/flash-attention) ä¸” PyTorch Version \<= 1.13        | No Flash Attention       |
| æœªå®‰è£… [flash attn](https://github.com/Dao-AILab/flash-attention) ä¸” 2.0 \<= PyTorch Version \<= 2.1 | Flash Attention 1        |
| æœªå®‰è£… [flash attn](https://github.com/Dao-AILab/flash-attention) ä¸” PyTorch Version >= 2.2          | Flash Attention 2        |

## å˜é•¿æ³¨æ„åŠ› (Variable Length Flash Attention)

### ç®€ä»‹

åœ¨[ç¬¬ä¸€èŠ‚](#æ•°æ®é›†æ‹¼æ¥)ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†â€œæ•°æ®é›†æ‹¼æ¥â€ç­–ç•¥å¯¹æ¨¡å‹è®­ç»ƒæ•ˆç‡çš„æ˜¾è‘—æå‡ã€‚ç†è®ºä¸Šï¼Œæ•°æ®é›†æ‹¼æ¥å¯èƒ½ä¼šå¯¹æ³¨æ„åŠ›ï¼ˆAttentionï¼‰æœºåˆ¶çš„è®¡ç®—è¿‡ç¨‹äº§ç”Ÿå½±å“ã€‚è¿™æ˜¯å› ä¸ºï¼Œåœ¨æœªé‡‡ç”¨æ•°æ®æ‹¼æ¥ç­–ç•¥çš„æƒ…å†µä¸‹ï¼Œæ¯æ¡æ•°æ®åœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶ä»…ä¸è‡ªèº«ç›¸å…³è”ã€‚ç„¶è€Œï¼Œå½“é‡‡ç”¨æ•°æ®æ‹¼æ¥ç­–ç•¥åï¼Œç”±å¤šæ¡çŸ­æ•°æ®æ‹¼æ¥æˆçš„é•¿æ•°æ®åœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶ä¼šç›¸äº’å…³è”ã€‚ä»¥ä¸€ä¸ªç”±è‹¥å¹²çŸ­æ•°æ®æ‹¼æ¥æˆé•¿åº¦ä¸º 4096 çš„æ•°æ®ä¸ºä¾‹ï¼Œå¦‚æœä¸é‡‡ç”¨å˜é•¿æ³¨æ„åŠ›æœºåˆ¶ï¼Œåœ¨æ³¨æ„åŠ›è®¡ç®—é˜¶æ®µï¼Œæ¯ä¸ª token å°†ä¼šå…³æ³¨å…¨éƒ¨ 4096 ä¸ª tokens ï¼Œå¦‚å›¾å·¦ä¾§æ‰€ç¤ºã€‚

ç›¸åï¼Œåœ¨ä½¿ç”¨å˜é•¿æ³¨æ„åŠ›æœºåˆ¶çš„æƒ…å†µä¸‹ï¼Œæ¯ä¸ª token åœ¨æ³¨æ„åŠ›è®¡ç®—é˜¶æ®µä»…ä¼šå…³æ³¨å…¶æ‰€åœ¨çŸ­æ•°æ®ä¸­çš„æ‰€æœ‰ tokens ï¼Œå¦‚å›¾å³ä¾§æ‰€ç¤ºã€‚å› æ­¤ï¼Œ**å˜é•¿æ³¨æ„åŠ›æœºåˆ¶ç¡®ä¿äº†æ— è®ºæ˜¯å¦é‡‡ç”¨â€œæ•°æ®é›†æ‹¼æ¥â€ç­–ç•¥ï¼Œæ¨¡å‹è®­ç»ƒçš„è¡Œä¸ºä¿æŒä¸€è‡´æ€§**ã€‚

<div align="center">
  <img src="https://github.com/InternLM/InternLM/assets/41630003/7e0c6a02-a970-4bd3-a10b-8341720bf654" width="600"/>
  <br /><br />
</div>

### XTuner å˜é•¿æ³¨æ„åŠ›æ”¯æŒæƒ…å†µ

> \[!IMPORTANT\]
> ä½¿ç”¨å˜é•¿æ³¨æ„åŠ›éœ€è¦é¦–å…ˆå®‰è£… [flash attn](https://github.com/Dao-AILab/flash-attention) ï¼ˆå‚è€ƒ [flash attn å®‰è£…](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features) ï¼‰

|   æ¨¡å‹    | Variable Length Flash Attention |
| :-------: | :-----------------------------: |
| baichuan  |  :negative_squared_cross_mark:  |
|  chatglm  |  :negative_squared_cross_mark:  |
| deepseek  |       :white_check_mark:        |
|   gemma   |  :negative_squared_cross_mark:  |
| internlm  |       :white_check_mark:        |
|   llama   |       :white_check_mark:        |
|  mistral  |       :white_check_mark:        |
|   qwen    |       :white_check_mark:        |
| starcoder |  :negative_squared_cross_mark:  |
|    yi     |       :white_check_mark:        |
|  zephyr   |       :white_check_mark:        |

### åœ¨ XTuner ä¸­ä½¿ç”¨å˜é•¿æ³¨æ„åŠ›æœºåˆ¶

#### Step 1, å®‰è£… flash_attn

XTuner ä¸­å®ç°çš„å˜é•¿æ³¨æ„åŠ›éœ€è¦ä¾èµ– Flash Attention 2ï¼Œå¯é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š

```bash
MAX_JOBS=4 pip install flash-attn --no-build-isolation
```

è¯¦ç»†å®‰è£…æ­¥éª¤è¯·å‚è€ƒ [flash attn å®‰è£…](https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features)

#### Step 2, åˆ—å‡ºå€™é€‰æ¨¡å‹åå­—

XTuner æä¾›å¤šä¸ªå¼€ç®±å³ç”¨çš„é…ç½®æ–‡ä»¶ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä¸‹åˆ—å‘½ä»¤æŸ¥çœ‹ï¼š

```bash
xtuner list-cfg -p internlm
```

`-p` ä¸ºæ¨¡ç³ŠæŸ¥æ‰¾ï¼Œè‹¥æƒ³è®­ç»ƒå…¶ä»–æ¨¡å‹ï¼Œå¯ä»¥ä¿®æ”¹ `internlm` ä¸º XTuner æ”¯æŒçš„å…¶ä»–æ¨¡å‹åç§°ã€‚

#### Step 3, å¤åˆ¶ config æ–‡ä»¶

å¯¼å‡ºéœ€è¦ä½¿ç”¨çš„ config ï¼š

```bash
xtuner copy-cfg ${CONFIG_NAME} ${SAVE_DIR}
```

ä¾‹å¦‚é€šè¿‡ä¸‹åˆ—å‘½ä»¤å°†åä¸º `internlm_7b_full_oasst1_e3` çš„ config å¯¼å‡ºè‡³å½“å‰ç›®å½•ä¸‹ï¼š

```bash
xtuner copy-cfg internlm_7b_full_oasst1_e3 .
```

#### Step 4, ä¿®æ”¹ config æ–‡ä»¶

å°† Step 3 å¤åˆ¶å¾—åˆ°çš„ config æ–‡ä»¶ä¸­çš„ `use_varlen_attn` å±æ€§ç”± False æ”¹ä¸º True å³å¯æ¿€æ´»å˜é•¿æ³¨æ„åŠ›è®­ç»ƒæœºåˆ¶ï¼š

```diff
...
#######################################################################
#                          PART 1  Settings                           #
#######################################################################
# Model
pretrained_model_name_or_path = 'internlm/internlm-7b'
- use_varlen_attn = False
+ use_varlen_attn = True
...
```

> \[!IMPORTANT\]
> éœ€è¦æ³¨æ„ï¼Œå½“è®¾ç½® `use_varlen_attn = True` åï¼Œè¯·ç¡®ä¿ `batch_size` è¢«è®¾ç½®ä¸º 1ï¼Œä¸” `pack_to_max_length` è¢«è®¾ç½®ä¸º Trueã€‚

#### Step 5, å¼€å§‹è®­ç»ƒ

```
xtuner train ${CONFIG_NAME_OR_PATH}
```

ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥åŸºäº Step 4 ä¸­ä¿®æ”¹å¾—åˆ°çš„ `internlm_7b_full_oasst1_e3_copy.py` è¿›è¡Œè®­ç»ƒï¼š

```bash
# On a single GPU
xtuner train internlm_7b_full_oasst1_e3_copy.py --deepspeed deepspeed_zero1
# On multiple GPUs
(DIST) NPROC_PER_NODE=${GPU_NUM} xtuner train internlm_7b_full_oasst1_e3_copy.py --deepspeed deepspeed_zero1
(SLURM) srun ${SRUN_ARGS} xtuner train internlm_7b_full_oasst1_e3_copy.py --launcher slurm --deepspeed deepspeed_zero1
```

- `--deepspeed` è¡¨ç¤ºä½¿ç”¨ [DeepSpeed](https://github.com/microsoft/DeepSpeed) ğŸš€ æ¥ä¼˜åŒ–è®­ç»ƒè¿‡ç¨‹ã€‚è‹¥æœªå®‰è£… DeepSpeed ï¼Œå¯é€šè¿‡ `pip install deepspeed>=0.12.3` è¿›è¡Œå®‰è£…ã€‚XTuner å†…ç½®äº†å¤šç§ç­–ç•¥ï¼ŒåŒ…æ‹¬ ZeRO-1ã€ZeRO-2ã€ZeRO-3 ç­‰ã€‚å¦‚æœç”¨æˆ·æœŸæœ›å…³é—­æ­¤åŠŸèƒ½ï¼Œè¯·ç›´æ¥ç§»é™¤æ­¤å‚æ•°ã€‚

#### Step 6, æ¨¡å‹è½¬æ¢

å°†ä¿å­˜çš„ PTH æ¨¡å‹ï¼ˆå¦‚æœä½¿ç”¨çš„DeepSpeedï¼Œåˆ™å°†ä¼šæ˜¯ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼‰è½¬æ¢ä¸º HuggingFace æ¨¡å‹ï¼š

```
xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH} ${SAVE_PATH}
```

å¯¹åº”ä¸Šé¢çš„ä¾‹å­ï¼Œæ¨¡å‹è½¬æ¢è„šæœ¬ä¸ºï¼š

```
xtuner convert pth_to_hf internlm_7b_full_oasst1_e3_copy.py ${PTH} ${SAVE_PATH}
```

å…¶ä¸­ `${PTH}` ä¸ºè®­ç»ƒæƒé‡ä¿å­˜çš„è·¯å¾„ï¼Œè‹¥æœªæŒ‡å®šï¼Œé»˜è®¤ä¿å­˜åœ¨ `./work_dirs/internlm_7b_full_oasst1_e3_copy` è·¯å¾„ä¸‹ã€‚
