# 概述

本节将向您介绍 XTuner 的整体框架和工作流程，并提供详细的教程链接。

## 什么是XTuner、

XTuner是由InternLM团队开发的一个高效、灵活且全能的轻量化大模型微调工具库。其主要用于多种大型语言模型的高效微调，包括大语言模型InternLM和多模态图文模型LLaVa。XTuner不仅提供了丰富的模型、数据集、数据管道和算法支持，还配备了现成的配置文件和快速入门指南，使得用户能够便捷地进行模型微调和部署。
此外，XTuner支持多种微调方法，如QLoRA和全量微调，可与DeepSpeed集成以优化训练过程。它还允许用户对微调后的模型进行评估和量化，从而在保证模型效能的同时提高其效率。总体来看，XTuner为大型语言模型的微调提供了一个高效、全面且用户友好的解决方案，适用于追求性能优化和定制化的开发者和研究者。

## XTuner的工作流程

  1. **数据采集及格式转换**：为了有效地进行模型微调，首先需要明确微调的具体目标并基于此目标进行数据采集。采集完成后，关键步骤是将数据集格式转换为模型支持的官方格式。这一过程中，XTuner支持将多种格式的数据集进行转换，包括HuggingFace等平台开源数据集及自定义的json格式数据集（需要按照支持的数据集格式进行构建）。
  2. **模型选择**：在数据准备就绪之后，下一步是根据具体需求选择合适的大型语言模型进行微调。XTuner平台支持多种主流的大型语言模型，并且用户还可以选择使用自己训练的模型，只要这些模型是通用的 HuggingFace 格式。
  3. **微调方法的选择**：在微调的过程中，目前主要支持的方法包括QLora、Lora以及全量微调（full tuning）。这些方法对于显存的要求各不相同，其中全量微调对显存的需求最高，而QLora微调的显存需求则相对较低。特别是对于7B规模的模型，使用QLora微调甚至可以在只有8GB显存的家用消费级显卡上进行，大大降低了硬件门槛，使得更多用户能够尝试对模型进行微调。
  4. **配置文件的创建**：开始时，我们可以通过执行xtuner list-cfg命令来列出所有支持的配置文件。这一步骤允许我们从众多选项中筛选出最适合我们任务需求的配置文件。例如，配置文件internlm2_chat_7b_qlora_oasst1_512_e3.py中包含了丰富的信息，如模型名称internlm2_chat_7b、微调方法qlora以及数据集oasst1等。基于此，我们可以根据之前确定的数据格式、模型和微调方法来选择合适的配置文件。选定配置文件后，我们可以使用xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}命令，将所选的配置文件下载到本地。
  5. **配置文件的修改**：下载配置文件到本地后，根据自身需求对其进行修改是必要的。最关键的修改包括更新模型的路径和数据集的路径。除此之外，根据不同的模型，可能需要调整模型参数和配置（model）。同时，根据所选数据集的不同，可能还需更改load_dataset函数和dataset_map_fn函数。此外，不同模型所对应的prompt_template（提示模板）也可能有所不同。在后续的章节中，我们将对配置文件的修改进行更详细的讨论和指导。
  6. **模型训练**：配置文件修改完毕后，我们便可以使用Xtuner train命令来启动模型的训练。在这个阶段，我们可以通过设置一些特定的参数，比如是否启用deepspeed等，来对训练过程进行细致调整。训练完成后，我们将获得多个.pth格式的模型权重文件以及记录了全部训练信息的.log文件。通过分析.log文件中的数据，我们可以利用特定的方法对模型训练过程中的学习率变化和损失值变化进行可视化处理，从而更直观地理解模型的训练进程。
  7. **模型转换及测试**：由于训练完成的模型默认格式为.pth文件，我们需要将其转换为huggingface格式，以便进行后续的对话测试及下游应用。这时，我们可以执行xtuner convert pth_to_hf命令来完成模型的格式转换。转换完成后，我们可以通过xtuner chat命令启动模型，并通过提出问题的方式来测试模型的性能和响应能力。
  8. **模型部署**：在模型测试完成后，XTuner还提供了基于LMDeploy的模型部署功能。我们可以使用python -m lmdeploy.pytorch.chat命令，传入相应的参数，来进行模型的部署工作。这样，我们的模型就可以在更广泛的环境中被应用，为更多的用户提供服务。


## XTuner的核心模块

### apis
- 提供了部分给第三方连接的接口（目前正在开发）

### Configs
- 存放着不同模型、不同数据集以及微调方法的配置文件
- 可以自行从huggingface上下载模型和数据集后进行一键启动

### Dataset
- 在`map_fns`下存放了支持的数据集的映射规则
- 在`collate_fns`下存放了关于数据整理函数部分的内容
- 提供了用于存放和加载不同来源数据集的函数和类

### Engine
- `hooks`中展示了哪些信息将会哪个阶段下在终端被打印出来。

### Evaluation
- `metric`中展示了XTuer所支持的一种评测数据集MMLU的格式，我们可以通过`XTuner test`进行调用

### Model
- `modules`中存放了对部分模型的优化策略，可以非侵入式地对模型进行修改。

### Tools
- 这里面是XTuner中的核心工具箱，里面存放了我们常用的打印config文件（`list_cfg`）、复制config文件(`copy_cfg`)、训练(`train`)以及对话(`chat`)等等。
- 在`model_converters`中也提供了模型转换以及切分的脚本
- 在`plugin`中提供了部分工具调用的函数


## XTuner当前支持的模型、数据集及微调方法

### 支持的大语言模型
XTuner目前支持以下大语言模型，可支持所有huggingface格式的大语言模型：
- `baichuan`
- `chatglm`
- `internlm`
- `llama`
- `llava`
- `mistral`
- `mixtral`
- `qwen`
- `yi`
- `starcoder`
- `zephyr`
- ...

### 支持的数据集格式
XTuner目前支持以下数据集格式：
- `alpaca`
- `alpaca_zh`
- `code_alpaca`
- `arxiv`
- `colors`
- `crime_kg_assistant`
- `law_reference`
- `llava`
- `medical`
- `msagent`
- `oasst1`
- `openai`
- `openorca`
- `pretrain`
- `sql`
- `stack_exchange`
- `tiny_codes`
- `wizardlm`
- ...

### 支持的微调方法
XTuner目前支持以下微调方法：
- `QLora`
- `Lora`
- `Full`
- ...
