# Minimum 2.16.0 to fix some bugs, see https://github.com/huggingface/datasets/pull/6444
datasets>=2.16.0
einop
# Avoid `import cv2` failed
opencv-python==4.7.0.72
# Minimum 0.10.3 to support distributed evaluation for MMBench
# see https://github.com/open-mmlab/mmengine/pull/1469
mmengine>=0.10.3
scikit-image
scipy
SentencePiece
tiktoken
torch
torchvision
# Minimum 4.36.0 to support `Cache` data structure used by KV Cache
# Registering a causal mask in `LlamaModel` is not friendly for very large
# `max_position_embeddings`. Refer to
# https://github.com/huggingface/transformers/blob/v4.38.0/src/transformers/models/llama/modeling_llama.py#L921-L923
# transformers >= 4.43.0 use _flash_attention_forward but not self._flash_attention_forward
# to calculate attn output which lead to bc braeking
transformers>=4.45
transformers_stream_generator
loguru
pydantic