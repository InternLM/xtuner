import uuid
from dataclasses import dataclass, field
from typing import List

import ray
import ray.util.queue

from transformers import AutoTokenizer
from xtuner.v1.ray.config.worker import RolloutConfig
from xtuner.v1.ray.dataflow import SampleParams

from .worker import RolloutWorker


@dataclass
class RolloutMeta:
    uid: str
    prompt: str
    input_ids: List[int]
    response: str
    output_ids: List[int]
    sample_params: SampleParams
    extra_params: dict = field(default_factory=dict)


@ray.remote
class RolloutController:
    def __init__(
        self,
        infer_config: RolloutConfig,
        workers: List[RolloutWorker],
        inqueue: ray.util.queue.Queue = None,
        outqueue: ray.util.queue.Queue = None,
    ):
        self.config = infer_config
        self.num_workers = 0
        self.worker_server_urls: List[str] = []
        self.active_rollout_workers: List[ray.actor.ActorHandle] = []
        self.inqueue = inqueue if inqueue is not None else ray.util.queue.Queue(maxsize=10)
        self.outqueue = outqueue if outqueue is not None else ray.util.queue.Queue(maxsize=1000)
        self.tokenizer = (
            AutoTokenizer.from_pretrained(infer_config.model_path, trust_remote_code=True)
            if infer_config.tokenizer_path
            else None
        )
        self.init_workers(self.config, workers)

    def init_workers(self, infer_config: RolloutConfig, workers: List[RolloutWorker]):
        active_servers_count, nodes_per_engine = self._get_active_servers_count(infer_config, len(workers))
        interval = len(workers) // active_servers_count
        self.active_rollout_workers = workers[::interval]
        self.num_workers = len(self.active_rollout_workers)

        print(f"self.active_rollout_workers: {self.active_rollout_workers}")
        # init dist_init_addr for each worker according to parallel settings
        init_dist_init_addrs = ray.get([worker.init_dist_port.remote() for worker in self.active_rollout_workers])  # type: ignore[attr-defined]
        dist_init_addrs = self._update_dist_init_addr(
            nodes_per_engine, init_dist_init_addrs, infer_config.tensor_parallel_size
        )
        # launch rollout servers
        self.worker_server_urls = ray.get(
            [
                worker.init.remote(infer_config, dist_init_addrs[i])  # type: ignore[attr-defined]
                for i, worker in enumerate(self.active_rollout_workers)
            ]
        )
        return [worker.rollout.remote(self.inqueue, self.outqueue) for worker in self.active_rollout_workers]  # type: ignore[attr-defined]

    def init_router(self, infer_config: RolloutConfig):
        self.active_rollout_workers[0].launch_router.remote(infer_config)

    # call workers functions
    def rollout(self, prompt: str, sample_params: SampleParams = SampleParams()):
        input_ids = self.tokenizer.encode(prompt) if self.tokenizer else []
        uid = str(uuid.uuid4())
        rollout_meta = RolloutMeta(
            uid=uid,
            prompt=prompt,
            input_ids=input_ids,
            response="",
            output_ids=[],
            sample_params=sample_params,
        )
        self.inqueue.put(rollout_meta)

    def pause(self):
        return [worker.pause.remote() for worker in self.active_rollout_workers]

    def get_worker_status(self):
        return [worker.get_status.remote() for worker in self.active_rollout_workers]

    # internal functions
    def _update_dist_init_addr(self, nodes_per_engine, dist_init_addrs, tp_size):
        if nodes_per_engine > 1:
            index = list(range(0, self.num_workers + 1, tp_size)) + [self.num_workers]
            for i in range(1, len(index)):
                dist_init_addrs[index[i - 1] : index[i]] = [dist_init_addrs[index[i - 1]]] * (index[i] - index[i - 1])
        return dist_init_addrs

    def _get_active_servers_count(self, infer_config: RolloutConfig, gpu_nums: int):
        # NOTEï¼šSince different inference engines have different launch methods,
        # the number of nodes contained in each engine is not consistent.
        # For example: sglang requires starting an inference engine for each node,
        # while lmdeploy and vllm does not. Therefore, we calculate the number
        # of active servers based on the configuration.
        support_cross_node_comm = infer_config.rollout_cross_node_comm
        gpus_per_node = infer_config.gpus_per_node
        tp_size = infer_config.tensor_parallel_size
        nodes_per_engine = 1 if support_cross_node_comm or tp_size < gpus_per_node else tp_size // gpus_per_node
        active_servers_count = int((gpu_nums // tp_size) * nodes_per_engine)
        return active_servers_count, nodes_per_engine

    def reset_prefix_cache(self):
        return [worker.reset_prefix_cache.remote() for worker in self.active_rollout_workers]

    def offload(self):
        return [worker.sleep.remote() for worker in self.active_rollout_workers]

    def onload(self):
        return [worker.wake_up.remote() for worker in self.active_rollout_workers]
